[["index.html", "Py-Book Preface Setup", " Py-Book Preface .github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}} This is a book aimed at summarizing my knowledge in python language Setup I mainly work with RStudio, therefore I won‚Äôt go deeper inside the usual Pyhton Setup, instead I just suggest to install the package reticulate and work with it within RStudio. #install.packages(&#39;reticulate&#39;) library(reticulate) # conda_list() # conda_version() virtualenv_create(envname = &quot;r-reticulate&quot;, packages = c(&#39;pandas&#39;, &#39;numpy&#39;, &#39;scikit-learn&#39;) ) virtualenv_list() virtualenv_exists() virtualenv_root() #------------------------------------- use_virtualenv(virtualenv_root() ) #repl_python() #quit Now Python is ready to use Here you can find everything you need to know on using reticulate package in r To clear all the environment # import sys # sys.modules[__name__].__dict__.clear() #Or delete single object #del &lt;variable_name&gt; "],["algorithms.html", "Chapter 1 Algorithms 1.1 Definitions 1.2 Computers Architecture", " Chapter 1 Algorithms 1.1 Definitions Algorithm is a non-ambiguous and repeatable sequence of instructions which allows to solve a problem in a general way. The instructions are as et of elementary operations which are assumed to be executable by a predefined executor. Where: Executor model a schematic description of internal architecture of the executor and its components Some components are numerical variables and expressions, assignment, conditionals, for loops, while loops, function definition and invocation. Expressions are constructed with parentheses, constants, scalar variables, arithmetic or logical operators, and follow the formation rules of arithmetic and logical expressions. We assume a countable set of symbols, called variables. A variable may be: undefined, that is, have no value, or scalar, that is,have a numerical or logical value, or vector, that is, have a finite sequence of values Every scalar variable in the expression is replaced by its value and all operations in the resulting expression are computed in the order set by the rules of arithmetic and logic, until the value of the expression is obtained. An assignment is a pair consisting of a scalar variable and an expression, separated by an assignment symbol. A conditional statement is an expression, which evaluates to either true or false,and two sequences of instructions. A for loop consists of a scalar variableùë•, a vector variableùë£, and a sequence of instructions. A while loop is an expression, which evaluates to either true or false, and asequence of instructions A function definition is a function name, a sequence of parameters, and a sequence of instructions, called the body of the function. A function invocation is a function name and a sequence of expressions. Set of operations the machine can execute and how the components interact to compute them Set of rules to write algorithms that use the machine operations, i.e., a language. 1.2 Computers Architecture Independent of the writing style, the recommended format for algorithm presentation consists of an Input section describing a generic instance, an Output section describing the corresponding solution, an Algorithm section listing the lines of code in the chosen style. The sequences of instructions in conditional statements, for and while loops must be indented. The Von Neumann architecture: a single memory stores both program and data. A central processing unit (CPU) executes program instructions. Arithmetic operations are computed by a sub-unit of the CPU, the arithmetic-logic unit (ALU). The CPU contains a small amount of memory in a collection of registers,which store the current instruction (instruction register, IR), the address in memory of the next instruction (program counter, PC), operands, memory addresses, and the status of the last executed instruction. A bidirectional bus connects the CPU to the memory. Input and output units are connected to the memory. "],["basics.html", "Chapter 2 Basics 2.1 Control Flow 2.2 Collection Types", " Chapter 2 Basics 2.1 Control Flow 2.1.1 Data Types int() converts to integers str() converts to strings float() converts to float (integers with decimals) There exist also booleans: True, False So what are the other comparison operators? Well, we‚Äôve got: equal to ==, not equal to !=, greater than &gt;, smaller than &lt;, greater or equal to &gt;=, smaller or equal to &lt;= Other booleans operators are and, or, not. Be careful not to confuse assignment (one equals sign) with comparison (two equals signs) 2.1.2 Variables A variable lets you store a value by assigning it to a name. The name can be used to refer to the value later in the program. You can use letters, numbers, and underscores in variable names. But you can‚Äôt use special symbols, or start the name with a number. x = 5 name = &quot;Alessio&quot; print(x,name) #&gt; 5 Alessio The input function prompts the user for input, and returns what they enter as a string. Like this: x = int(input(&quot;Insert a value for x:&quot;)) Remember to specify the type of the value in this case int was used in order to work with an integer value for x #### In-Place Operators In-place operators let you write code like `x = x + 3¬¥ more concisely, as ‚Äòx += 3‚Äô, for all the operations &lt;+,-,*,/,%,**,//&gt; 2.1.3 Logics 2.1.3.1 if, else, elif ifstatements to run code based on a certain condition The else statement can be used to run some statements when the condition of the if statement is not met. elif is the short for else if statement, used when chaining several if statements if condition: statement elif condition: statement: else condition: statement Python uses indentation (that empty space at the beginning of a line) to delimit blocks of code. Depending on the program‚Äôs logic, indentation can be mandatory. As you can see, the statements in the if should be indented. 2.1.3.2 while We can use the while loop to repeat a block of code multiple times. For example, let‚Äôs say we need to process multiple user inputs, so that each time the user inputs something, the same block of code needs to execute. To end a while loop prematurely, we can use a break statement. Instead continue jumps back to the top of the loop, rather than stopping it. Basically, the continue statement stops the current iteration and continues with the next one. 2.1.4 Iteration The for loop is used to iterate over a given sequence, such as lists or strings. string = &quot;testing for loops&quot; count = 0 #initialize counter for i in string: if(i == &#39;t&#39;): count += 1 print(count) #How many letters &quot;t&quot;? #&gt; 2 while or for? Usually we‚Äôd use the for loop when the number of iterations is fixed. For example, iterating over a fixed list of items in a shopping list. The while loop is useful in cases when the number of iterations isn‚Äôt known and depends on some calculations and conditions in the code block of the loop. Inserting %%time in the code chunk reports the computational time to complete the operation 2.1.5 Functions Functions are defined with the def statement. The statement ends with a colon, and the code that is part of the function is indented below the def statement. def function(x): return(x) function(5) #&gt; 5 2.1.5.1 Functional Programming In this section I will define higher-order functions. These are functions that have no side effects, and return a value that depend only on the argument and therefore it doesn‚Äôt change the structure of functions or elements within. def pure_function(x,y): temp = x + 2*y return temp / (2*x + y) pure_function(2,1) #&gt; 0.8 easier to test and reasoning more efficient. It reduces the number of times the function is called (memoization) easier to run in parallel Python allows to have function with varying number of arguments. Using *args (the name args is just a convention; you can choose to use another) as a function parameter enables you to pass an arbitrary number of arguments to that function. The arguments are then accessible as the tuple args in the body of the function. The parameter *args must come after the named parameters to a function. Also, named parameters to a function can be made optional by giving them a default value. These must come after named parameters without a default value. def function(x, *args, food=&#39;egg&#39;): print(x, args, food) function(1,2,3) #reports the default value of food #&gt; 1 (2, 3) egg function(1,2,3,4,5, food=&#39;salmon&#39;) #&gt; 1 (2, 3, 4, 5) salmon 2.1.5.1.1 Lambdas Python allows to create functions on-the-fly (anonymous), thanks to lambda() syntax. However they are not as powerful as takes a single line of code #Named Functions def polynomial(x): return x**2 + 5*x + 4 print(polynomial(1)) #&gt; 10 #Lambda print((lambda x: x**2 + 5*x + 4) (1)) #argument:(1) #&gt; 10 2.1.5.1.2 map and filter These two are high-order functions that operate on lists or iterables. - The function map() takes a function and an iterable as arguments and returns a new iterable with the function applied to each argument def add_five(x): return x+5 nums = list(range(0,11)) result = list(map(add_five,nums)) print(result) #&gt; [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] The filter() function filters an iterable, leaving only the objects that match a condition (predicate) nums = [11,22,33,44,55] #return even number from the list res = list(filter(lambda x: x%2==0, nums)) print(res) #&gt; [22, 44] 2.1.5.1.3 Generators Generators are a type of iterable. They are created by the yield statement, which replaces the return of a function to provide a result without destroying local variables. Moreover as they yield one item at time, generators don‚Äôt have memory restrictions and so can be infinite. They can be converted into list with the function list(). def countdown(): i=5 while i &gt; 0: yield i i -= 1 print(list(countdown())) #&gt; [5, 4, 3, 2, 1] Its usage results in improved performance, as consume low memory and there is no need to wait until all elements have been generated before starting using them 2.2 Collection Types 2.2.1 Lists At their simplest, Lists are used to store items. We can create a list by using square brackets with commas separating items. Like this: words = [&quot;Hello&quot;, &quot;world&quot;, &quot;!&quot;] print(words[0]) #&gt; Hello print(words[1]) #&gt; world print(words[2]) #or: #&gt; ! print(words[0]+ &quot; &quot; + words[1] + words[2]) #&gt; Hello world! Indexing a string is like creating a list containing each character in the string. string = &quot;Hello world!&quot; print(string[0]) #First element is indexed w/ 0 #&gt; H print(&quot;H&quot; in string) #&gt; True print(&quot;Hello&quot; not in string) #&gt; False Lists can also be added and multiplied in the same way as strings. nums = [1, 2, 3] print(nums + [4, 5, 6]) #&gt; [1, 2, 3, 4, 5, 6] print(nums * 3) #&gt; [1, 2, 3, 1, 2, 3, 1, 2, 3] 2.2.1.1 range() This function, if converted to a list, creates number sequences of the form: \\([a,b) \\ \\ | \\ a \\le x &lt;b\\), i.e. the argument won‚Äôt be included in the list #To produce an object from 0 to first argument print(list(range(10))) #&gt; [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] #To produce an object from 1st to 2nd argument(not included) print(list(range(1,4))) #&gt; [1, 2, 3] A third argument can be added if you want to include steps #From 0 to 10 by step of 2 (even numbers) print(list(range(0,10,2))) #&gt; [0, 2, 4, 6, 8] 2.2.1.2 List Slices Other way to retrieve values from a list: using : numbers = list(range(0,11)) print(numbers[:6]) #numbers from 1st to 5th #&gt; [0, 1, 2, 3, 4, 5] print(numbers[5:]) #from the 5th to last #&gt; [5, 6, 7, 8, 9, 10] #Retrieve elements from list by step of 2 print(numbers[::2]) #&gt; [0, 2, 4, 6, 8, 10] #Retrieve elements from 1st argument to nth last one using - print(numbers[1:-2]) #from 2nd to 3rd last one #&gt; [1, 2, 3, 4, 5, 6, 7, 8] There is a way to remove an item from a list given its index instead of its value: the *del* statement. This differs from the pop() method which returns a value. The del statement can also be used to remove slices from a list or clear the entire list #delete numbers from 2 to 9 in numbers list del numbers[2:10] numbers #&gt; [0, 1, 10] 2.2.1.3 List Comprehensions This is a way to creating lists whose contents obey a rule evens = [i**2 for i in range(10) if i**2 % 2 == 0] print(evens) #&gt; [0, 4, 16, 36, 64] 2.2.1.4 List Functions len(): Gets you the number of items in a list (or a string) .append() Adds an element at the end of the list .clear() Removes all the elements from the list .copy() Returns a copy of the list .count() Returns the number of elements with the specified value .extend() Add the elements of a list (or any iterable), to the end of the current list .index() Returns the index of the first element with the specified value .insert() Adds an element at the specified position .pop() Removes the element at the specified position .remove() Removes the first item with the specified value .reverse() Reverses the order of the list .sort() Sorts the list 2.2.1.5 Strings Function .join- joins a list of strings with another string as a separator. replace- replaces one substring in a string with another. .startswithand .endswith - determine if there is a substring at the start and end of a string, respectively. .lowerand .upper‚Äì changes the case of a string .split - the opposite of .join, turns a string with a certain separator into a list. .find() - Searches the string for a specified value and returns the position of where it was found .format() - Formats specified values in a string .format_map() - Formats specified values in a string .index() - Searches the string for a specified value and returns the position of where it was found .isalpha() - Returns True if all characters in the string are in the alphabet .swapcase() - Swaps cases, lower case becomes upper case and vice versa .title() - Converts the first character of each word to upper case .translate() - Returns a translated string 2.2.2 Matrixes We can use nested lists to represent 2D grids, such as matrices. m = [ [1, 2, 3], [4, 5, 6] ] #The code below outputs the 3rd item of the 2nd row. print(m[1][2]) #&gt; 6 However it‚Äôs mainly used the NumPy (Chapter 4 is a package for scientific computing which has support for a powerful N-dimensional array object. Before you can use NumPy, you need to install it. For more info, 2.2.3 Dictionaries Another useful data type built into Python is the dictionary. Dictionaries are sometimes found in other languages as ‚Äúassociative memories‚Äù or ‚Äúassociative arrays‚Äù. Unlike sequences, which are indexed by a range of numbers, dictionaries are indexed by keys, which can be any immutable type; strings and numbers can always be keys. prefix = {&#39;italy&#39;: 39, &#39;spain&#39;: 34, &#39;france&#39;: 33} prefix[&#39;italy&#39;] #&gt; 39 The dict() constructor builds dictionaries directly from sequences of key-value pairs dict([(&#39;sape&#39;, 4139), (&#39;guido&#39;, 4127), (&#39;jack&#39;, 4098)]) #&gt; {&#39;sape&#39;: 4139, &#39;guido&#39;: 4127, &#39;jack&#39;: 4098} 2.2.4 Tuples Tuples are immutable sequences, typically used to store collections of heterogeneous data. Tuples may be constructed in a number of ways: Using a pair of parentheses to denote the empty tuple: () Using a trailing comma for a singleton tuple: a, or (a,) Separating items with commas: a, b, c or (a, b, c) Using the tuple() built-in: tuple() or tuple(iterable) 2.2.5 Sets "],["text-mining.html", "Chapter 3 Text Mining 3.1 Working with strings 3.2 Regular Expressions", " Chapter 3 Text Mining 3.1 Working with strings import this 3.1.1 String Types new_string = &quot;This is a String&quot; # storing a string print(f&#39;ID: {id(new_string)}&#39;) # shows the object identifier (address) #&gt; ID: 4775069456 print(f&#39;Type: {type(new_string)}&#39;) # shows the object type (type(new_string) #&gt; Type: &lt;class &#39;str&#39;&gt; print(f&#39;Value: {new_string}&#39;) # shows the object value #&gt; Value: This is a String simple_string = &#39;Hello!&#39; + &quot; I&#39;m a simple string&quot; print(simple_string) # multi-line string, note the \\n (newline) escape character automatically created #&gt; Hello! I&#39;m a simple string multi_line_string = &quot;&quot;&quot;Hello I&#39;m a multi-line string!&quot;&quot;&quot; multi_line_string #&gt; &quot;Hello I&#39;m\\na multi-line\\nstring!&quot; print(multi_line_string) #&gt; Hello I&#39;m #&gt; a multi-line #&gt; string! Be careful when writing raw strings, regex will escape them. # Normal string with escape sequences leading to a wrong file path! escaped_string = &quot;C:\\the_folder\\new_dir\\file.txt&quot; print(escaped_string) #&gt; C: he_folder #&gt; ew_dir ile.txt In order to keep the backslashes, use r'' to define a raw string: # raw string keeping the backslashes in its normal form raw_string = r&#39;C:\\the_folder\\new_dir\\file.txt&#39; print(raw_string) #&gt; C:\\the_folder\\new_dir\\file.txt 3.1.2 Strings Operations # concatenation of variables and literals s1 = &#39;Python üíª!&#39; &#39;Hello üòä &#39; + s1 #&gt; &#39;Hello üòä Python üíª!&#39; s1*5 #&gt; &#39;Python üíª!Python üíª!Python üíª!Python üíª!Python üíª!&#39; # concatenating several strings together in parentheses s3 = (&#39;This &#39; &#39;is another way &#39; &#39;to concatenate &#39; &#39;several strings!&#39;) s3 #&gt; &#39;This is another way to concatenate several strings!&#39; Checking for substrings in a string: &#39;way&#39; in s3, &#39;python&#39; in s3 #&gt; (True, False) Computing total length of the string: len(s3) #&gt; 51 3.1.3 Strings Conversion s = &#39;python is great&#39; s.capitalize(), s.upper(), s.title(), s.replace(&#39;python&#39;, &#39;R&#39;) #&gt; (&#39;Python is great&#39;, &#39;PYTHON IS GREAT&#39;, &#39;Python Is Great&#39;, &#39;R is great&#39;) Checking for numbers and alphabet #.isdecimal() checks only for numeric strings &#39;12345&#39;.isdecimal(), &#39;apollo11&#39;.isdecimal() #.isalpha() checks only for numeric strings #&gt; (True, False) &#39;python&#39;.isalpha(), &#39;number1&#39;.isalpha() #.isalpha() checks for alphanumeric strings, not just numeric strings #&gt; (True, False) &#39;total&#39;.isalnum(),&#39;abc123&#39;.isalnum(), &#39;1+1&#39;.isalnum() #&gt; (True, True, False) 3.1.4 String Indexing and Slicing s = &#39;PYTHON&#39; for index, character in enumerate(s): print(f&#39;Character -&gt; {character} has index-&gt; {index}&#39;) #&gt; Character -&gt; P has index-&gt; 0 #&gt; Character -&gt; Y has index-&gt; 1 #&gt; Character -&gt; T has index-&gt; 2 #&gt; Character -&gt; H has index-&gt; 3 #&gt; Character -&gt; O has index-&gt; 4 #&gt; Character -&gt; N has index-&gt; 5 s[0], s[1], s[2], s[3], s[4], s[5] #&gt; (&#39;P&#39;, &#39;Y&#39;, &#39;T&#39;, &#39;H&#39;, &#39;O&#39;, &#39;N&#39;) s[:], s[1:4], s[:3], s[3:], s[-3:], s[:3] + s[-3:] #&gt; (&#39;PYTHON&#39;, &#39;YTH&#39;, &#39;PYT&#39;, &#39;HON&#39;, &#39;HON&#39;, &#39;PYTHON&#39;) 3.1.5 String splitting and joining s = &#39;I,am,a,comma,separated,string&#39; s.split(&#39;,&#39;) #&gt; [&#39;I&#39;, &#39;am&#39;, &#39;a&#39;, &#39;comma&#39;, &#39;separated&#39;, &#39;string&#39;] &#39; &#39;.join(s.split(&#39;,&#39;)) #&gt; &#39;I am a comma separated string&#39; # stripping whitespace characters s = &#39; I am surrounded by spaces &#39; s, s.strip() #&gt; (&#39; I am surrounded by spaces &#39;, &#39;I am surrounded by spaces&#39;) s = &#39;Python is great. NLP is also good.&#39; s.split(&#39;.&#39;) #&gt; [&#39;Python is great&#39;, &#39; NLP is also good&#39;, &#39;&#39;] print(&#39;\\n&#39;.join(s.split(&#39;.&#39;))) #&gt; Python is great #&gt; NLP is also good print(&#39;\\n&#39;.join([s.strip() for s in s.split(&#39;.&#39;) if s])) #&gt; Python is great #&gt; NLP is also good 3.2 Regular Expressions Regular expressions are a powerful tool for various kinds of string manipulation. They are a domain specific language (DSL) that is present as a library in most modern programming languages, not just Python. Regular expressions in Python can be accessed using the re module, which is part of the standard library. import re s1 = &#39;Python is an excellent language&#39; s2 = &#39;I love the Python language. I also use Python to build applications at work!&#39; 3.2.1 Match &amp; Find The match() function only returns a match if a match is found at the beginning of the string s1. pattern = &#39;python&#39; re.match(pattern, s1), re.match(pattern, s1, flags=re.IGNORECASE) #&gt; (None, &lt;re.Match object; span=(0, 6), match=&#39;Python&#39;&gt;) pattern is in lower case, hence ignore case flag helps in matching same pattern with different cases. Other functions to match patterns are re.search() and re.findall(). The function re.search() finds a match of a pattern anywhere in the string. The function re.findall() returns a list of all substrings that match a pattern. re.search(pattern, s2, re.IGNORECASE) #&gt; &lt;re.Match object; span=(11, 17), match=&#39;Python&#39;&gt; re.findall(pattern, s2, re.IGNORECASE) #&gt; [&#39;Python&#39;, &#39;Python&#39;] There is also re.finditer() which does the same, except it returns an iterator rather than a list print(f&#39;String: {s2}&#39;) #&gt; String: I love the Python language. I also use Python to build applications at work! for m in re.finditer(pattern, s2, re.IGNORECASE): print(f&quot;Found match &#39;{m.group(0)}&#39; ranging from index {m.start()} - {m.end()}&quot;) #&gt; Found match &#39;Python&#39; ranging from index 11 - 17 #&gt; Found match &#39;Python&#39; ranging from index 39 - 45 The regex search returns an object with several methods that give details about it. These methods include group which returns the string matched, start and end which return the start and ending positions of the first match. print(&#39;Found match {} ranging from index {} - {} in the string &quot;{}&quot;&#39;.format( re.match(pattern, s1, flags=re.IGNORECASE).group(0), re.match(pattern, s1, flags=re.IGNORECASE).start(), re.match(pattern, s1, flags=re.IGNORECASE).end(), s1)) #&gt; Found match Python ranging from index 0 - 6 in the string &quot;Python is an excellent language&quot; 3.2.2 Search &amp; Replace One of the most important re methods that use regular expressions is .sub(). This method replaces all occurrences of the pattern in string with repl, substituting all occurrences, unless count provided. This method returns the modified string. re.sub(pattern, &#39;R&#39;, s2, flags=re.IGNORECASE) #&gt; &#39;I love the R language. I also use R to build applications at work!&#39; 3.2.3 Metacharacters Metacharacters are what make regular expressions more powerful than normal string methods. They allow you to create regular expressions to represent concepts like ‚Äúone or more repetitions of a vowel‚Äù. The existence of metacharacters poses a problem if you want to create a regular expression (or regex) that matches a literal metacharacter, such as ‚Äú$‚Äù. You can do this by escaping the metacharacters by putting a backslash in front of them. However, this can cause problems, since backslashes also have an escaping function in normal Python strings. This can mean putting three or four backslashes in a row to do all the escaping. 3.2.3.1 Example: Email Extraction To demonstrate a sample usage of regular expressions, lets create a program to extract email addresses from a string. Suppose we have a text that contains an email address: str = \"Please contact info@sololearn.com for assistance\". Our goal is to extract the substring ‚Äúinfo@sololearn.com‚Äù. A basic email address consists of a word and may include dots or dashes. This is followed by the @ sign and the domain name (the name, a dot, and the domain name suffix). This is the basis for building our regular expression. pattern = r&quot;([\\w\\.-]+)@([\\w\\.-]+)(\\.[\\w\\.]+)&quot; str = &quot;Please contact info@sololearn.com for assistance&quot; match = re.search(pattern, str) if match: print(match.group()) #&gt; info@sololearn.com [\\w\\.-]+ matches one or more word character, dot or dash. The regex above says that the string should contain a word (with dots and dashes allowed), followed by the @ sign, then another similar word, then a dot and another word. "],["numpy.html", "Chapter 4 Numpy 4.1 Arrays 4.2 Operators", " Chapter 4 Numpy Numpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays 4.1 Arrays A numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the array. The shape of an array is a tuple of integers giving the size (the total number of elements) of the array along each dimension. import numpy as np # 1-dimensonal array, also referred to as a vector a1 = np.array([1, 2, 3]) a1.shape, a1.ndim, a1.dtype, a1.size, type(a1) #&gt; ((3,), 1, dtype(&#39;int64&#39;), 3, &lt;class &#39;numpy.ndarray&#39;&gt;) # 2-dimensional array, also referred to as matrix a2 = np.array([[1, 2.0, 3.3], [4, 5, 6.5]]) a2.shape, a2.ndim, a2.dtype, a2.size, type(a2) #&gt; ((2, 3), 2, dtype(&#39;float64&#39;), 6, &lt;class &#39;numpy.ndarray&#39;&gt;) # 3-dimensional array, also referred to as a matrix a3 = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]]]) a3.shape, a3.ndim, a3.dtype, a3.size, type(a3) #&gt; ((2, 3, 3), 3, dtype(&#39;int64&#39;), 18, &lt;class &#39;numpy.ndarray&#39;&gt;) 4.1.1 Creating Arrays np.array() np.ones() np.full() np.zeros() np.random.rand(5, 3) np.random.randint(10, size=5) np.random.seed() - pseudo random numbers You can change the data type with .astype() or calling the argument dtype= when creating. # Create an array of ones, 10 rows of 2 cols type integer ones = np.ones((10, 2)) # Create an array of zeros zeros = np.zeros((5, 3, 3)) # One-dim array from 1 to 10 a = np.arange(1,11) # One-dim array of 100 numbers from 1 to 10 a = np.linspace(1,11,50) # Random array of integers a = np.random.randint(10, size=(5, 3)) # Random array of floats (between 0 &amp; 1) a = np.random.random((5, 3)) For consistency, you might want to keep the random numbers you generate similar throughout experiments. To do this, you can use np.random.seed(). 4.1.2 Indexing and slicing arrays Array shapes are always listed in the format (row, column, n...) where n is optional extra dimension(s). a = np.arange(100) a = a.reshape(50,2) a[0,1] # same form of an access to a nested list or a[0][1] #&gt; 1 a[1:3] # row slice #&gt; array([[2, 3], #&gt; [4, 5]]) a[:,0] # project on first column #&gt; array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, #&gt; 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, #&gt; 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98]) a[0] # first row or a[0, :] #&gt; array([0, 1]) a[:3,0] # Get the first value of the first 3 row #&gt; array([0, 2, 4]) b = np.arange(48).reshape(4, 12) b, b[(0,2), 2:5], b[:, (-1, 2, -1)] # returns a 1D array with b[-1, 5], b[2, 9], b[-1, 1] and b[2, 9] #&gt; (array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], #&gt; [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], #&gt; [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], #&gt; [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]), array([[ 2, 3, 4], #&gt; [26, 27, 28]]), array([[11, 2, 11], #&gt; [23, 14, 23], #&gt; [35, 26, 35], #&gt; [47, 38, 47]])) b[(-1, 2, -1, 2), (5, 9, 1, 9)] #&gt; array([41, 33, 37, 33]) 4.1.3 Reshape a = np.arange(0,11) ## Reshape into n rows of n col # Row vector a = a.reshape((1,11)) #a[np.newaxis, :] a #&gt; array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]) # Column vector a = a.reshape((11,1)) #a[:, np.newaxis] a #&gt; array([[ 0], #&gt; [ 1], #&gt; [ 2], #&gt; [ 3], #&gt; [ 4], #&gt; [ 5], #&gt; [ 6], #&gt; [ 7], #&gt; [ 8], #&gt; [ 9], #&gt; [10]]) # Transpose the array a.T #&gt; array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]) 4.1.3.1 Aggregate #Concatenate (The arrays must have equal numbers of dimensions) a=np.array([1,2,3]) b=np.array([4,5,6]) c=np.array([7,8,9]) np.concatenate([a, b, c]) #&gt; array([1, 2, 3, 4, 5, 6, 7, 8, 9]) For arrays with different numbers of dimensions, use vstack and hstack to perform concatenation. The inverse operation, splitting, is implemented by split,vsplit,hsplit a = np.array([2,3,5]) b = np.array([[7,11,13], [17,19,23]]) np.vstack([a, b]) #np.concatenate((a, b), axis=0) #&gt; array([[ 2, 3, 5], #&gt; [ 7, 11, 13], #&gt; [17, 19, 23]]) b = np.array([[29], [31]]) b2 = np.array([[7,11,13], [17,19,23]]) np.hstack([b, b2]) ##np.concatenate((a, b), axis=1) #&gt; array([[29, 7, 11, 13], #&gt; [31, 17, 19, 23]]) a = np.array([2,3,5,7,11,13,17,19,23]) a1, a2, a3 = np.split(a, [3,6]) a1, a2, a3 #&gt; (array([2, 3, 5]), array([ 7, 11, 13]), array([17, 19, 23])) There is also a split function which splits an array along any given axis. Calling vsplit is equivalent to calling split with axis=0. There is also an hsplit function, equivalent to calling split with axis=1. 4.1.3.2 Sort np.sort() np.argsort() np.argmax() np.argmin() 4.1.3.3 Transpose The transpose method creates a new view on an ndarray‚Äôs data, with axes permuted in the given order. By default, transpose reverses the order of the dimensions: a = np.arange(24).reshape(4,2,3) a #&gt; array([[[ 0, 1, 2], #&gt; [ 3, 4, 5]], #&gt; #&gt; [[ 6, 7, 8], #&gt; [ 9, 10, 11]], #&gt; #&gt; [[12, 13, 14], #&gt; [15, 16, 17]], #&gt; #&gt; [[18, 19, 20], #&gt; [21, 22, 23]]]) a2= a.transpose() # equivalent to t.transpose((2, 1, 0)) Now let‚Äôs create an ndarray such that the axes 0, 1, 2 (depth, height, width) are re-ordered to 1, 2, 0 (depth‚Üíwidth, height‚Üídepth, width‚Üíheight): a1 = a.transpose((1,2,0)) a1, a1.shape #&gt; (array([[[ 0, 6, 12, 18], #&gt; [ 1, 7, 13, 19], #&gt; [ 2, 8, 14, 20]], #&gt; #&gt; [[ 3, 9, 15, 21], #&gt; [ 4, 10, 16, 22], #&gt; [ 5, 11, 17, 23]]]), (2, 3, 4)) NumPy provides a convenience function swapaxes() to swap two axes. For example, let‚Äôs create a new view of t with depth and height swapped: a3 = a.swapaxes(0,1) # equivalent to a.transpose((1, 0, 2)) a3 #&gt; array([[[ 0, 1, 2], #&gt; [ 6, 7, 8], #&gt; [12, 13, 14], #&gt; [18, 19, 20]], #&gt; #&gt; [[ 3, 4, 5], #&gt; [ 9, 10, 11], #&gt; [15, 16, 17], #&gt; [21, 22, 23]]]) The T attribute is equivalent to calling transpose() when the rank is 2. The T attribute has no effect on rank 0 (empty) or rank 1 arrays. a.T #&gt; array([[[ 0, 6, 12, 18], #&gt; [ 3, 9, 15, 21]], #&gt; #&gt; [[ 1, 7, 13, 19], #&gt; [ 4, 10, 16, 22]], #&gt; #&gt; [[ 2, 8, 14, 20], #&gt; [ 5, 11, 17, 23]]]) 4.2 Operators All the usual arithmetic operators can be used with ndarrays. They apply element wise: * Arithmetic * +, -, *, /, //, **, % * np.exp() * np.log() * np.dot() - Dot product The arrays must have the same shape. If they do not, NumPy will apply the broadcasting rules. 4.2.1 Broadcasting Rules If the arrays do not have the same rank, then a 1 will be prepended to the smaller ranking arrays until their ranks match. h = np.arange(5).reshape(1, 1, 5) h #&gt; array([[[0, 1, 2, 3, 4]]]) Now let‚Äôs try to add a 1D array of shape (5,) to this 3D array of shape (1,1,5). Applying the first rule of broadcasting! h + [10, 20, 30, 40, 50] #same as: h + [[[10, 20, 30, 40, 50]]] #&gt; array([[[10, 21, 32, 43, 54]]]) Arrays with a 1, along a particular dimension, act as if they had the size of the array with the largest shape along that dimension. The value of the array element is repeated along that dimension. k = np.arange(6).reshape(2,3) k #&gt; array([[0, 1, 2], #&gt; [3, 4, 5]]) Let‚Äôs try to add a 2D array of shape (2,1) to this 2D ndarray of shape (2, 3). NumPy will apply the second rule of broadcasting: k + [[100], [200]] # same as: k + [[100, 100, 100], [200, 200, 200]] #&gt; array([[100, 101, 102], #&gt; [203, 204, 205]]) Combining rules 1 &amp; 2, we can do this: k + [100, 200, 300] # after rule 1: [[100, 200, 300]], and after rule 2:‚ê£ Ùè∞∞‚Üí[[100, 200, 300], [100, 200, 300]] #&gt; array([[100, 201, 302], #&gt; [103, 204, 305]]) After rules 1 &amp; 2, the sizes of all arrays must match. try: k + [33, 44] except ValueError as e: print(e) #&gt; operands could not be broadcast together with shapes (2,3) (2,) 4.2.2 Upcasting When trying to combine arrays with different dtypes, NumPy will upcast to a type capable of handling all possible values (regardless of what the actual values are). k1 = np.arange(0, 5, dtype=np.uint8) print(k1.dtype, k1) #&gt; uint8 [0 1 2 3 4] k2 = k1 + np.array([5, 6, 7, 8, 9], dtype=np.int8) # Note that int16 is required to represent all possible int8 and uint8 values (from -128 to 255) print(k2.dtype, k2) #&gt; int16 [ 5 7 9 11 13] k3 = k1 + 1.5 print(k3.dtype, k3) #&gt; float64 [1.5 2.5 3.5 4.5 5.5] 4.2.3 Conditional operators The conditional operators also apply elementwise. Comparison operators &gt; &lt; &lt;= &gt;= x != 3 x == 3 np.sum(x &gt; 3) k2 &lt; np.array([2, 8, 11, 9, 6]) #&gt; array([False, True, True, False, False]) k2 &lt; 8 #&gt; array([ True, True, False, False, False]) 4.2.4 Statistics Many mathematical and statistical functions are available for ndarrays. Aggregation np.sum() - faster than .sum() np.prod() np.mean() np.std() np.var() np.min() np.max() np.argmin() - find index of minimum value np.argmax() - find index of maximum value These functions accept an optional argument axis which lets you ask for the operation to be performed on elements along the given axis. For example: c=np.arange(24).reshape(2,3,4) c #&gt; array([[[ 0, 1, 2, 3], #&gt; [ 4, 5, 6, 7], #&gt; [ 8, 9, 10, 11]], #&gt; #&gt; [[12, 13, 14, 15], #&gt; [16, 17, 18, 19], #&gt; [20, 21, 22, 23]]]) c.sum(axis=0) # sum across matrices #&gt; array([[12, 14, 16, 18], #&gt; [20, 22, 24, 26], #&gt; [28, 30, 32, 34]]) c.sum(axis=1) # sum across rows #&gt; array([[12, 15, 18, 21], #&gt; [48, 51, 54, 57]]) c.sum(axis=(0,2)) # sum across matrices and columns (first row, second row , 8+9+10+11 + 20+21+22+23) #&gt; array([ 60, 92, 124]) 4.2.5 Linear Algebra Linear algebra functions are available in the numpy.linalg module. The inv function computes a square matrix‚Äôs inverse, whereas the pinv() computes the pseudoinverse. import numpy.linalg as linalg a = np.array([[1,2,3],[5,7,11],[21,29,31]]) linalg.inv(a), linalg.pinv(a) #&gt; (array([[-2.31818182, 0.56818182, 0.02272727], #&gt; [ 1.72727273, -0.72727273, 0.09090909], #&gt; [-0.04545455, 0.29545455, -0.06818182]]), array([[-2.31818182, 0.56818182, 0.02272727], #&gt; [ 1.72727273, -0.72727273, 0.09090909], #&gt; [-0.04545455, 0.29545455, -0.06818182]])) The qr function computes the QR decomposition of a matrix, also known as a QR factorization or QU factorization. It is a decomposition of a matrix A into a product A = QR of an orthogonal matrix Q and an upper triangular matrix R. q, r = linalg.qr(a) #q.dot(r) -&gt; q**r equals a The det() function computes the matrix determinant. linalg.det(a) #&gt; 43.99999999999997 The eig() function from linalg computes the eigenvalues and eigenvectors of a square matrix: eigenvalues, eigenvectors = linalg.eig(a) eigenvalues, eigenvectors #&gt; (array([42.26600592, -0.35798416, -2.90802176]), array([[-0.08381182, -0.76283526, -0.18913107], #&gt; [-0.3075286 , 0.64133975, -0.6853186 ], #&gt; [-0.94784057, -0.08225377, 0.70325518]])) The diag() function returns the elements on the diagonal, whereas trace() returns their sum: np.diag(a),np.trace(a) #&gt; (array([ 1, 7, 31]), 39) The solve function solves a system of linear scalar equations, such as: \\[2x+6y=6\\] \\[5x+3y=-9\\] coeffs = np.array([[2, 6], [5, 3]]) depvars = np.array([6, -9]) solution = linalg.solve(coeffs, depvars) solution #&gt; array([-3., 2.]) "],["pandas.html", "Chapter 5 Pandas 5.1 Series 5.2 DataFrame", " Chapter 5 Pandas A powerful library for data manipulation and analysis, borrowing the idea of data frames from the R language. An essential import setup for Pandas is import pandas as pd from datetime import datetime, date 5.1 Series Similar to one-dimensional NumPy array, with the addition of an index which can comprisearbitrary values pd.Series([2,5,6,7]) #&gt; 0 2 #&gt; 1 5 #&gt; 2 6 #&gt; 3 7 #&gt; dtype: int64 In the example above, the index has not been specified, and it defaults to the standard 0-based integer index. s = pd.Series([2,5,6,7],index=(&quot;prime1&quot;,&quot;prime2&quot;,&quot;prime3&quot;,&quot;prime4&quot;)) print(s) #&gt; prime1 2 #&gt; prime2 5 #&gt; prime3 6 #&gt; prime4 7 #&gt; dtype: int64 And then indexes can be used for slicing s[[&quot;prime2&quot;,&quot;prime4&quot;]] #&gt; prime2 5 #&gt; prime4 7 #&gt; dtype: int64 When instantiated from a dictionary input, the keys are used to create the index s = pd.Series({&#39;Friday&#39;:0.2,&#39;Saturday&#39;:1.0,&#39;Sunday&#39;:4.2,&#39;Monday&#39;:0.0,&#39;Tuesday&#39;:1.2}) print(s[1:3]) #&gt; Saturday 1.0 #&gt; Sunday 4.2 #&gt; dtype: float64 The overall behaviour of Series is similar to that of ndarray, as far as slicing and indexing are concerned: def fahrenheit(t): return 9*t/5+32 s[s&lt;1.2], #&gt; (Friday 0.2 #&gt; Saturday 1.0 #&gt; Monday 0.0 #&gt; dtype: float64,) fahrenheit(s) #&gt; Friday 32.36 #&gt; Saturday 33.80 #&gt; Sunday 39.56 #&gt; Monday 32.00 #&gt; Tuesday 34.16 #&gt; dtype: float64 Series has some features of dict, like assignment by index and testing of index value membership s[&#39;Monday&#39;] = np.nan pd.isna(s[&#39;Monday&#39;]) #&gt; True dt=pd.date_range(&#39;2019-10-29&#39;,&#39;2019-11-02&#39;) print(dt) #&gt; DatetimeIndex([&#39;2019-10-29&#39;, &#39;2019-10-30&#39;, &#39;2019-10-31&#39;, &#39;2019-11-01&#39;, #&gt; &#39;2019-11-02&#39;], #&gt; dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) precip_sp = pd.Series([0.2,1.0,4.2,0.0,1.2],index=dt) print(precip_sp) #&gt; 2019-10-29 0.2 #&gt; 2019-10-30 1.0 #&gt; 2019-10-31 4.2 #&gt; 2019-11-01 0.0 #&gt; 2019-11-02 1.2 #&gt; Freq: D, dtype: float64 precip_bu = pd.Series([0.0,0.0,0.0,1.2,4.2,0.0], index=pd.date_range(&#39;2019-10-27&#39;,&#39;2019-11-01&#39;)) #NaN as precip_bu and precip_sp have different indexes range print(precip_sp-precip_bu) #&gt; 2019-10-27 NaN #&gt; 2019-10-28 NaN #&gt; 2019-10-29 0.2 #&gt; 2019-10-30 -0.2 #&gt; 2019-10-31 0.0 #&gt; 2019-11-01 0.0 #&gt; 2019-11-02 NaN #&gt; Freq: D, dtype: float64 5.2 DataFrame A2-dimensional, table-like, data structure with columns of possibly different types and an index for its rows. You can create a DataFrame by passing a dictionary of Series objects: people_dict = { &quot;weight&quot;: pd.Series([68, 83, 112], index=[&quot;alice&quot;, &quot;martino&quot;, &quot;franco&quot;]), &quot;birthyear&quot;: pd.Series([1984, 1985, 1992], index=[&quot;alice&quot;, &quot;martino&quot;,&quot;franco&quot;], name=&quot;year&quot;), &quot;children&quot;: pd.Series([0, 3], index=[&quot;martino&quot;, &quot;franco&quot;]), &quot;hobby&quot;: pd.Series([&quot;Biking&quot;, &quot;Dancing&quot;], index=[&quot;alice&quot;, &quot;martino&quot;]), } people = pd.DataFrame(people_dict) people #&gt; weight birthyear children hobby #&gt; alice 68 1984 NaN Biking #&gt; franco 112 1992 3.0 NaN #&gt; martino 83 1985 0.0 Dancing Another convenient way to create a DataFrame is to pass all the values to the constructor as an ndarray, or a list of lists, and specify the column names and row index labels separately: values = [ [1985, np.nan, &quot;Biking&quot;, 68], [1984, 3, &quot;Dancing&quot;, 83], [1992, 0, np.nan, 112] ] d = pd.DataFrame( values, columns=[&quot;birthyear&quot;, &quot;children&quot;, &quot;hobby&quot;, &quot;weight&quot;], index=[&quot;alice&quot;, &quot;martino&quot;, &quot;franco&quot;]) d #&gt; birthyear children hobby weight #&gt; alice 1985 NaN Biking 68 #&gt; martino 1984 3.0 Dancing 83 #&gt; franco 1992 0.0 NaN 112 precips = pd.DataFrame({&#39;San Pietro Capofiume&#39;: precip_sp,&#39;Bologna Urbana&#39;: precip_bu}) print(precips) #&gt; San Pietro Capofiume Bologna Urbana #&gt; 2019-10-27 NaN 0.0 #&gt; 2019-10-28 NaN 0.0 #&gt; 2019-10-29 0.2 0.0 #&gt; 2019-10-30 1.0 1.2 #&gt; 2019-10-31 4.2 4.2 #&gt; 2019-11-01 0.0 0.0 #&gt; 2019-11-02 1.2 NaN Panda‚Äôs DataFrame() function is flexible as to the types of its inputs. As a list of dictionaries, each dictionary element of the input list is one row ofthe data frame city_loc = pd.DataFrame([{&#39;Lat&#39;:44.49381,&#39;Long&#39;:11.33875}, {&#39;Lat&#39;:41.89193,&#39;Long&#39;:12.51133}], index=[&#39;Bologna&#39;,&#39;Rome&#39;]) print(city_loc) #&gt; Lat Long #&gt; Bologna 44.49381 11.33875 #&gt; Rome 41.89193 12.51133 DataFrame behaves like a dictionary, in which keys are column names and the values are index-aligned Series city_loc.columns #&gt; Index([&#39;Lat&#39;, &#39;Long&#39;], dtype=&#39;object&#39;) city_loc[&#39;Lat&#39;] #&gt; Bologna 44.49381 #&gt; Rome 41.89193 #&gt; Name: Lat, dtype: float64 # add an elevation column for example as a list city_loc[&#39;Elev&#39;] = [54,21] city_loc #&gt; Lat Long Elev #&gt; Bologna 44.49381 11.33875 54 #&gt; Rome 41.89193 12.51133 21 5.2.1 Indexing 5.2.1.1 .loc[] attribute Label-based indexing is supported via the attribute .loc[], the result is a Series object. precips.loc[&#39;2019-10-30&#39;], #&gt; (San Pietro Capofiume 1.0 #&gt; Bologna Urbana 1.2 #&gt; Name: 2019-10-30 00:00:00, dtype: float64,) type(precips.loc[&#39;2019-10-30&#39;]) #&gt; &lt;class &#39;pandas.core.series.Series&#39;&gt; .loc[] supports slicing. However, unlike ndarray integer-based slicing,it includes the upper label: precips.loc[&#39;2019-10-29&#39;:&#39;2019-11-01&#39;] #&gt; San Pietro Capofiume Bologna Urbana #&gt; 2019-10-29 0.2 0.0 #&gt; 2019-10-30 1.0 1.2 #&gt; 2019-10-31 4.2 4.2 #&gt; 2019-11-01 0.0 0.0 and it also allows for list of labels: # Note: in this example the list has to be transformed into a DatetimeIndex because the Series objects which were the input of the data frame had been constructed with a DatetimeIndex precips.loc[pd.DatetimeIndex([&#39;2019-10-31&#39;,&#39;2019-11-01&#39;,&#39;2019-11-02&#39;])] #&gt; San Pietro Capofiume Bologna Urbana #&gt; 2019-10-31 4.2 4.2 #&gt; 2019-11-01 0.0 0.0 #&gt; 2019-11-02 1.2 NaN Is it possible to index through booleans #create index with missing values na=(pd.isna(precips[&#39;Bologna Urbana&#39;])|pd.isna(precips[&#39;San Pietro Capofiume&#39;])) #index full dataset without(-) values which are na precips.loc[~na] #&gt; San Pietro Capofiume Bologna Urbana #&gt; 2019-10-29 0.2 0.0 #&gt; 2019-10-30 1.0 1.2 #&gt; 2019-10-31 4.2 4.2 #&gt; 2019-11-01 0.0 0.0 5.2.1.2 .iloc[] attribute The.iloc attribute provides integer-based indexing, following the usual Pythonconventions. The allowed types of input are the same as.loc, but integers mustbe specified instead of labels. The following examples return the same resultsas the ones above: precips.iloc[3] #&gt; San Pietro Capofiume 1.0 #&gt; Bologna Urbana 1.2 #&gt; Name: 2019-10-30 00:00:00, dtype: float64 precips.iloc[2:6] #&gt; San Pietro Capofiume Bologna Urbana #&gt; 2019-10-29 0.2 0.0 #&gt; 2019-10-30 1.0 1.2 #&gt; 2019-10-31 4.2 4.2 #&gt; 2019-11-01 0.0 0.0 precips.iloc[[4,5,6]] #&gt; San Pietro Capofiume Bologna Urbana #&gt; 2019-10-31 4.2 4.2 #&gt; 2019-11-01 0.0 0.0 #&gt; 2019-11-02 1.2 NaN Is it possible to index with booleans, however first we need to convert it to a numpy array wiht .to_numpy() precips.iloc[~na.to_numpy()] #&gt; San Pietro Capofiume Bologna Urbana #&gt; 2019-10-29 0.2 0.0 #&gt; 2019-10-30 1.0 1.2 #&gt; 2019-10-31 4.2 4.2 #&gt; 2019-11-01 0.0 0.0 5.2.1.3 .query() The .query() method lets you filter a DataFrame based on a query expression; the best choice if you have several conditions to be applied on the row selection. df.query(&quot;(column1 == &#39;value&#39;) and (column2 &gt; 1000)&quot;) n = 200 df.query(&quot;(column1 == &#39;value&#39;) and (column2 &gt; @n)&quot;) df.query(f&quot;(column1 == &#39;value&#39;) or (column2 &gt; {n})&quot;) 5.2.2 Reading Data 5.2.2.1 read_csv # iris1 = pd.read_csv(&quot;https://raw.githubusercontent.com/SakshamAyush/Iris_Flower_Classification/master/data/bezdekIris.csv&quot;) The compression argument enables to specify the decompression algorithm if dataset is a compressed CSV file, setting the value 'infer' chooses it by file extension. By default, time series are not recognized, the dtypeof the series is object, it should be datetime64[ns]. the parse_dates argument enables us to specify a list of columns that must be parse as dates (i.e. parse_dates = ['year'] or parse_dates = [0]) The index_col argument enables us to set a sequence of columns as row labels (ex. index_col='Time') 5.2.2.2 read_excel The read_excel function enables the reading of .xls and .xlsx files. Installation of either xlrd or openpyxl (for Excel 2007,.xlsx) is a prerequisite. 5.2.3 Data Wrangling Function pd.concat on a list of objects can concatenate Series or DataFrame objects #Row labels are not modified s1=pd.Series([1,2,3]) s2=pd.Series([4,5,6]) s=pd.concat([s1, s2]) s #&gt; 0 1 #&gt; 1 2 #&gt; 2 3 #&gt; 0 4 #&gt; 1 5 #&gt; 2 6 #&gt; dtype: int64 Be careful when concatenating DataFrame objects. Let c1 and c2 be the column labels of the 1st and 2nd frame Columnsùê∂2 and ùê∂1 are added to first data frame, columnsùê∂1 and ùê∂2 to the second, all filled with NaN. The rows of the second frame are appended to the first df1=pd.DataFrame({&#39;a&#39;: [1,2,3],&#39;b&#39;: [4,5,6]}) df2=pd.DataFrame({&#39;b&#39;: [7,8,9],&#39;c&#39;: [10,11,12]}) df=pd.concat([df1, df2]) 5.2.3.1 Multi-indexing If all columns are tuples of the same size, then they are understood as a multi-index. The same goes for row index labels. For example: d = pd.DataFrame( { (&quot;public&quot;, &quot;birthyear&quot;): {(&quot;Paris&quot;,&quot;alice&quot;):1985, (&quot;Paris&quot;,&quot;martino&quot;): 1984, (&quot;London&quot;,&quot;franco&quot;):1992}, (&quot;public&quot;, &quot;hobby&quot;): {(&quot;Paris&quot;,&quot;alice&quot;):&quot;Biking&quot;, (&quot;Paris&quot;,&quot;martino&quot;): &quot;Dancing&quot;}, (&quot;private&quot;, &quot;weight&quot;): {(&quot;Paris&quot;,&quot;alice&quot;):68, (&quot;Paris&quot;,&quot;martino&quot;): 83, (&quot;London&quot;,&quot;franco&quot;): 112}, (&quot;private&quot;, &quot;children&quot;): {(&quot;Paris&quot;, &quot;alice&quot;):np.nan, (&quot;Paris&quot;,&quot;martino&quot;): 3, (&quot;London&quot;,&quot;franco&quot;): 0} } ) d #&gt; public private #&gt; birthyear hobby weight children #&gt; Paris alice 1985 Biking 68 NaN #&gt; martino 1984 Dancing 83 3.0 #&gt; London franco 1992 NaN 112 0.0 You can transpose columns and indices using the .T attribute: dT = d.T There are two levels of columns, and two levels of indices. We can drop a column level by calling droplevel() (the same goes for indices): dT.columns = dT.columns.droplevel(level = 0) dT #&gt; alice martino franco #&gt; public birthyear 1985 1984 1992 #&gt; hobby Biking Dancing NaN #&gt; private weight 68 83 112 #&gt; children NaN 3.0 0.0 5.2.3.2 Stacking and unstacking levels Calling the stack() method will push the lowest column level after the lowest index: d.stack() #&gt; private public #&gt; Paris alice birthyear NaN 1985 #&gt; hobby NaN Biking #&gt; weight 68.0 NaN #&gt; martino birthyear NaN 1984 #&gt; children 3.0 NaN #&gt; hobby NaN Dancing #&gt; weight 83.0 NaN #&gt; London franco birthyear NaN 1992 #&gt; children 0.0 NaN #&gt; weight 112.0 NaN dT.stack() #&gt; public birthyear alice 1985 #&gt; martino 1984 #&gt; franco 1992 #&gt; hobby alice Biking #&gt; martino Dancing #&gt; private weight alice 68 #&gt; martino 83 #&gt; franco 112 #&gt; children martino 3.0 #&gt; franco 0.0 #&gt; dtype: object Note that many NaN values appeared. This makes sense because many new combinations did not exist before (eg. there was no bob in London). Calling unstack() will do the reverse, once again creating many NaN values. dT.unstack() #&gt; alice ... franco #&gt; birthyear children hobby weight ... birthyear children hobby weight #&gt; private NaN NaN NaN 68 ... NaN 0.0 NaN 112 #&gt; public 1985 NaN Biking NaN ... 1992 NaN NaN NaN #&gt; #&gt; [2 rows x 12 columns] The stack() and unstack() methods let you select the level to stack/unstack. You can even stack/unstack multiple levels at once, specifying the level parameter. 5.2.3.3 Adding or removing columns people #&gt; weight birthyear children hobby #&gt; alice 68 1984 NaN Biking #&gt; franco 112 1992 3.0 NaN #&gt; martino 83 1985 0.0 Dancing people[&quot;age&quot;] = 2018 - people[&quot;birthyear&quot;] # adds a new column &quot;age&quot; people[&quot;over 30&quot;] = people[&quot;age&quot;] &gt; 30 # adds another column &quot;over 30&quot; birthyears = people.pop(&quot;birthyear&quot;) del people[&quot;children&quot;] people #&gt; weight hobby age over 30 #&gt; alice 68 Biking 34 True #&gt; franco 112 NaN 26 False #&gt; martino 83 Dancing 33 True When you add a new column, it must have the same number of rows. Missing rows are filled with NaN, and extra rows are ignored. When adding a new column, it is added at the end (on the right) by default. You can also insert a column anywhere else using the .insert() method. people.insert(1, &quot;height&quot;, [172, 181, 185]) (people .assign(body_mass_index = lambda df: df[&quot;weight&quot;] / (df[&quot;height&quot;] / 100)** 2) .assign(overweight = lambda df: df[&quot;body_mass_index&quot;] &gt; 25) ) #&gt; weight height hobby age over 30 body_mass_index overweight #&gt; alice 68 172 Biking 34 True 22.985398 False #&gt; franco 112 181 NaN 26 False 34.186991 True #&gt; martino 83 185 Dancing 33 True 24.251278 False 5.2.3.4 Sorting a DataFrame You can sort a DataFrame by calling its sort_index method. By default it sorts the rows by their index label, in ascending order, but let‚Äôs reverse the order: people.sort_index(ascending=False) #&gt; weight height hobby age over 30 #&gt; martino 83 185 Dancing 33 True #&gt; franco 112 181 NaN 26 False #&gt; alice 68 172 Biking 34 True Note that sort_index returned a sorted copy of the DataFrame. To modify people directly, we can set the inplace= argument to True. Also, we can sort the columns instead of the rows by setting axis=1 or by the values instead of the labels, we can use sort_values() and specify the column to sort by: people.sort_values(by=&quot;age&quot;, inplace=True) people #&gt; weight height hobby age over 30 #&gt; franco 112 181 NaN 26 False #&gt; martino 83 185 Dancing 33 True #&gt; alice 68 172 Biking 34 True 5.2.3.5 Aggregating Similar to the SQL language, pandas allows grouping your data into groups to run calculations over each group. * .groupby(\" \") Pandas supports spreadsheet-like pivot tables that allow quick data summarization. To illustrate this, let‚Äôs create a simple DataFrame: 5.2.4 Evaluating an expression A great feature supported by pandas is expression evaluation. This relies on the numexpr library which must be installed. people.eval(&quot;weight / (height/100) ** 2 &gt; 25&quot;) #&gt; franco True #&gt; martino False #&gt; alice False #&gt; dtype: bool Assignment expressions are also supported. Let‚Äôs set inplace=True to directly modify the DataFrame rather than getting a modified copy: people.eval(&quot;body_mass_index = weight / (height/100) ** 2&quot;, inplace=True) people #&gt; weight height hobby age over 30 body_mass_index #&gt; franco 112 181 NaN 26 False 34.186991 #&gt; martino 83 185 Dancing 33 True 24.251278 #&gt; alice 68 172 Biking 34 True 22.985398 5.2.5 Handling Missing Data Dealing with missing data is a frequent task when working with real life data. Pandas offers a few tools to handle missing data. .isnull(), .isna(), indicate whether values are missing (True/False). Its sum (.isnull().sum(0)) reports and overview the number of NAs for each column. df[df.isnull().any(axis=1)] keeps only the rows that contain at least one null grades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]]) grades = pd.DataFrame(grades_array, columns=[&quot;sep&quot;, &quot;oct&quot;, &quot;nov&quot;], index=[&quot;alice&quot;,&quot;bob&quot;,&quot;charles&quot;,&quot;darwin&quot;]) grades #&gt; sep oct nov #&gt; alice 8 8 9 #&gt; bob 10 9 9 #&gt; charles 4 8 2 #&gt; darwin 9 10 10 bonus_array = np.array([[0,np.nan,2],[np.nan,1,0],[0, 1, 0], [3, 3, 0]]) bonus_points = pd.DataFrame(bonus_array, columns=[&quot;oct&quot;, &quot;nov&quot;, &quot;dec&quot;], index=[&quot;bob&quot;,&quot;colin&quot;, &quot;darwin&quot;, &quot;charles&quot;]) bonus_points #&gt; oct nov dec #&gt; bob 0.0 NaN 2.0 #&gt; colin NaN 1.0 0.0 #&gt; darwin 0.0 1.0 0.0 #&gt; charles 3.0 3.0 0.0 grades + bonus_points #&gt; dec nov oct sep #&gt; alice NaN NaN NaN NaN #&gt; bob NaN NaN 9.0 NaN #&gt; charles NaN 5.0 11.0 NaN #&gt; colin NaN NaN NaN NaN #&gt; darwin NaN 11.0 10.0 NaN Looks like the addition worked in some cases but way too many elements are now empty. That‚Äôs because when aligning the DataFrames, some columns and rows were only present on one side, and thus they were considered missing on the other side (NaN). Then adding NaN to a number results in NaN, hence the result. 5.2.5.1 .fillna() Let‚Äôs try to fix the problem above. For example, we can decide that missing data should result in a zero, instead of NaN. We can replace all NaN values by a any value using the fillna() method: (grades + bonus_points).fillna(0) #fill with 0s #&gt; dec nov oct sep #&gt; alice 0.0 0.0 0.0 0.0 #&gt; bob 0.0 0.0 9.0 0.0 #&gt; charles 0.0 5.0 11.0 0.0 #&gt; colin 0.0 0.0 0.0 0.0 #&gt; darwin 0.0 11.0 10.0 0.0 It‚Äôs a bit unfair that we‚Äôre setting grades to zero in September, though. Perhaps we should decide that missing grades are missing grades, but missing bonus points should be replaced by zeros: better_bonus_points = bonus_points.copy() better_bonus_points.insert(0, &quot;sep&quot;, 0) better_bonus_points.loc[&quot;alice&quot;] = 0 better_bonus_points = better_bonus_points.interpolate(axis=1) final_grades = grades + better_bonus_points final_grades #&gt; dec nov oct sep #&gt; alice NaN 9.0 8.0 8.0 #&gt; bob NaN 10.0 9.0 10.0 #&gt; charles NaN 5.0 11.0 4.0 #&gt; colin NaN NaN NaN NaN #&gt; darwin NaN 11.0 10.0 9.0 5.2.5.2 .dropna() So let‚Äôs call the .dropna() method to get rid of rows that are full of NaNs: final_grades_clean = final_grades.dropna(how=&quot;all&quot;) final_grades_clean #&gt; dec nov oct sep #&gt; alice NaN 9.0 8.0 8.0 #&gt; bob NaN 10.0 9.0 10.0 #&gt; charles NaN 5.0 11.0 4.0 #&gt; darwin NaN 11.0 10.0 9.0 To remove columns that are full of NaNs, we set the axis argument to 1: final_grades_clean = final_grades_clean.dropna(axis=1, how=&quot;all&quot;) final_grades_clean #&gt; nov oct sep #&gt; alice 9.0 8.0 8.0 #&gt; bob 10.0 9.0 10.0 #&gt; charles 5.0 11.0 4.0 #&gt; darwin 11.0 10.0 9.0 5.2.6 Data Manipulation .tonumeric() convert each column value to numeric .astype(str) convert, for example, to str the values .groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=NoDefault.no_default, observed=False, dropna=True) a groupby operation involves some combination of splitting the object, applying a function, and combining the results. This can be used to group large amounts of data and compute operations on these groups. df.groupby(&#39;column1&#39;).sum() Use .cut() function to create a vector with categories labeled according to some values. For example, suppose we have a variable ‚Äòmedian_income‚Äô and we want to split it into 5 categories, in a variable ‚Äòincome_cat‚Äô : df[&quot;income_cat&quot;] = pd.cut(housing[&quot;median_income&quot;], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[&#39;low income&#39;, &#39;low-mid income&#39;, &#39;mid income&#39;, &#39;mid-high income&#39;, &#39;high income&#39;])] 5.2.7 Overview Functions The .info() method prints out a summary of each columns contents: the .describe() method gives a nice overview of the main aggregated values over each column: * count: number of non-null (not NaN) values * mean: mean of non-null values * std: standard deviation of non-null values * min: minimum of non-null values * 25%, 50%, 75%: 25th, 50th and 75th percentile of non-null values * max: maximum of non-null values In order to know the characteristics of the elements, the .unique() function reports the single elements. Instead, .nunique() it counts the unique values. For object type of data, there may exits multiple categories (factors). To explore them use .value_counts() .shape() it reports the shape of the table (rows x columns) .corr(), compute pairwise correlation of columns, excluding NA/null values "],["data-visualization.html", "Chapter 6 Data Visualization 6.1 Matplotlib 6.2 Exploratory Visualization 6.3 Other Packages", " Chapter 6 Data Visualization 6.1 Matplotlib import matplotlib.pyplot as plt img = np.empty((20,30,3)) img[:, :10] = [0, 0.6, 0] img[:, 10:20] = [1, 1, 1] img[:, 20:] = [0.6, 0, 0] plt.imshow(img) plt.show() 6.2 Exploratory Visualization Suppose we want to have a look at the distribution of our dataset df = pd.read_csv(&#39;data/housing.csv&#39;) df.hist(bins=50, figsize=(20,15)) plt.show() Detect how each attribute correlates with the scatter_matrix() function, applied to a subset of attributes. It draws a matrix of scatter plot. from pandas.plotting import scatter_matrix attributes = [&quot;median_house_value&quot;, &quot;median_income&quot;, &quot;total_rooms&quot;, &quot;housing_median_age&quot;] scatter_matrix(df[attributes], figsize=(12, 8)) plt.show() 6.3 Other Packages 6.3.1 Seaborn 6.3.2 Bokeh 6.3.3 Plotly "],["machine-learning-with-scikit-learn.html", "Chapter 7 Machine Learning with Scikit-Learn 7.1 Data PreProcessing 7.2 Select and Train a Model", " Chapter 7 Machine Learning with Scikit-Learn 7.1 Data PreProcessing 7.1.1 Create a Training and Test Sets train_test_split(), this function randomly splits the dataset in training and test sets. random_state allows you to set the random generator seed and to make this notebook‚Äôs output identical at every run. from sklearn.model_selection import train_test_split train_set, test_set = train_test_split(df, test_size=0.2, random_state=42) StratifiedShuffleSplit(), this algorithm provides train and test indexes to split (stratify) data in train and test sets. For example based on a column of my dataset df from sklearn.model_selection import StratifiedShuffleSplit split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) for train_index, test_index in split.split(df, df[&quot;column&quot;]): strat_train_set = df.loc[train_index] strat_test_set = df.loc[test_index] Is it possible to compare the different proportions based on the split, in the overall dataset, the test set generated with the stratified sampling, in a test set generated using random sampling def income_cat_proportions(data): return data[&quot;column&quot;].value_counts() / len(data) train_set, test_set = train_test_split(df, test_size=0.2, random_state=42) compare_props = pd.DataFrame({ &quot;Overall&quot;: income_cat_proportions(df), &quot;Stratified&quot;: income_cat_proportions(strat_test_set), &quot;Random&quot;: income_cat_proportions(test_set)} ).sort_index() compare_props[&quot;Rand. %Error&quot;] = 100 * compare_props[&quot;Random&quot;] / compare_props[&quot;Overall&quot;] - 100 compare_props[&quot;Strat. %Error&quot;] = 100 * compare_props[&quot;Stratified&quot;] / compare_props[&quot;Overall&quot;] - 100 compare_props The test set generated using stratified sampling has category proportions almost identical to those in the full dataset. Remove the income_cat. for set_ in (strat_train_set, strat_test_set): set_.drop(&quot;income_cat&quot;, axis=1, inplace=True) After choosing the best technique to split the dataset (for example, in this case the stratified sampling), redefine your df Remember to separate the predictors (i.e median_house_value) by the labels # drop labels for training set df = strat_train_set.drop(&quot;median_house_value&quot;, axis=1) df_labels = strat_train_set[&quot;median_house_value&quot;].copy() 7.1.2 Impute Missing Values Machine Learning algorithms do not work with missing values. Use SimpleImputer() function to replace na values with the median values. The strategy parameter allows you to specify the imputation strategy. Remember also that imputation techniques doesn‚Äôt apply to categorical variables, so make sure you remove them from your df. from sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy=&quot;median&quot;) imputer.strategy # &#39;median&#39; #You can fit the imputer instance to the training data with the fit() function imputer.fit(df) #Imputer computes the median to the numerical attributes and stores results in statistics_ variable, which is equal to df.median().values: imputer.statistics_ #At this point you can define the training set by replacing the missing values with the median values of the learned medians. The result (X) is a numpy array containing the transformed features. You get the same results by applying fit_transform() X = imputer.transform(df) df_tr = pd.DataFrame(X, columns= df.columns, index= df.index) 7.1.3 Handling Texts and Categorical Attributes Now let‚Äôs preprocess the categorical input feature. We need to convert these categories from text to numbers. Let‚Äôs define an arbitrary dataframe containing the values of our categorical column df_cat = df[[\"categorical_column\"]] 7.1.3.1 Ordinal Encoder Try OrdinalEncoder() function that encodes categorical features as an integer array. By using the OrdinalEncoder approach ML techniques will assume that two nearby values are more similar than two distant values. Be careful as it may not be the case of your study; for example, if categories were encoded like this 0=Low-Income and 4=Low/Mid-Income, notice that these are more similar than 0=Low-Income and 1=High-Income df_cat = df[[&quot;categorical_column&quot;]] from sklearn.preprocessing import OrdinalEncoder ordinal_encoder = OrdinalEncoder() df_cat_encoded = ordinal_encoder.fit_transform(df_cat) df_cat_encoded[:10] #to see the df with encoded variables np.unique(df_cat_encoded) #to see the encodings #The ordinal_encoder stores the list of categories in its categories_ variable. It is a list of 1D array of categories for each categorical variable: (in this case we have just 1 categorical attribute): ordinal_encoder.categories_ 7.1.3.2 One Hot Encoder The OneHotEncoder() function encodes categorical features as a one-hot numeric array. It allows you to create a binary attribute per category. For example: one attribute is equal to 1 when category is Low-Income, 0 otherwise; one attribute is equal to 1 when category is Mid-Income, 0 otherwise; and so on This approach is called one-hot encoding because, for the corresponding row, only one attribute will be equal to 1, while the others will be 0. from sklearn.preprocessing import OneHotEncoder 1hot_encoder = OneHotEncoder() df_cat_1hot = 1hot_encoder.fit_transform(df_cat) df_cat_1hot By default, the OneHotEncoder() returns a sparse matrix. Each row is full of 0s except for a single 1 per row. This can consume tons of memory, you can only store the location of the nonzero elements. You can convert the sparse matrix to a dense array, if needed, by calling the toarray()function. df_cat_1hot.toarray() # Alternatively, you can set sparse=False when creating the OneHotEncoder: 1hot_encoder = OneHotEncoder(sparse=False) df_cat_1hot = 1hot_encoder.fit_transform(df_cat) df_cat_1hot # You can list the encoder&#39;s categories_ variable. cat_encoder.categories_ 7.1.4 Feature Scaling Some Machine Learning techniques do not perform well when the input have different scales. Two simple ways to get all attributes with the same scale are: min-max scaling or normalization: all values are shifted and rescaled so that they end up ranging from 0 to 1. First subtract the min value and then divide by the max minus the min. This approach is supported by MinMaxScaler() class. standardization: first it subtracts the mean value and then it divides by the standard deviation. This approach is supported by StandardScaler() class. You need to apply this procedure to the training data only. 7.1.5 Pipelines The Pipeline class helps with the data cleaning steps. Basically we can define the workflow that we‚Äôve applied previously , within few simple lines of code. Let‚Äôs build a pipeline for preprocessing the numerical attributes of the dataframe first: df_num = df.drop(&quot;categorical_variable_if_any&quot;, axis=1) from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler num_pipeline = Pipeline([ (&#39;imputer&#39;, SimpleImputer(strategy=&quot;median&quot;)), (&#39;attribs_adder&#39;, CombinedAttributesAdder()), (&#39;std_scaler&#39;, StandardScaler()) ]) df_num_tr = num_pipeline.fit_transform(df_num) The Pipeline class takes a list of name,estimator pairs defining a sequence steps. The names must be unique and do not contain __. The estimators are transformers. The num_pipleline object calls fit_transform() sequentially on all transformers, passing the output of each call as the parameter to the next call until it reaches the final estimator. Now let‚Äôs build a pipeline for preprocessing both the numerical attributes and categorical attributes. Use ColumnTransformer class that applies transformers to columns of an array or pandas DataFrame. The ColumnTransformer() function requires a list of tuples, where each tuple contains a name, a transformer and a list of names (or indexes) of columns that the transformer should be applied to. The full_pipeline object applies a fit_transform() to the df data. from sklearn.compose import ColumnTransformer num_attribs = list(df_num) cat_attribs = [&quot;categorical_variable_if_any&quot;] full_pipeline = ColumnTransformer([ #numerical attributes transformed with num_pipeline (&quot;num&quot;, num_pipeline, num_attribs), #categorical variables transformed with onehotencoder (&quot;cat&quot;, OneHotEncoder(), cat_attribs), ]) df_prepared = full_pipeline.fit_transform(df) 7.2 Select and Train a Model The flowchart below is designed to give users a bit of a rough guide on how to approach problems with regard to which estimators to try on your data 7.2.1 Training and Evaluating on the Training Set Let‚Äôs first train a Linear Regression model df = strat_train_set.drop(&quot;median_house_value&quot;, axis=1) # drop labels for training set df_labels = strat_train_set[&quot;median_house_value&quot;].copy() from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(df_prepared, df_labels) Let‚Äôs try the full preprocessing pipeline on a few training instances: the first 5 rows: some_data = df.iloc[:5] some_labels = df_labels.iloc[:5] some_data_prepared = full_pipeline.transform(some_data) print(&#39;Compare predictions against the actual values.&#39;) print(&quot;Predictions:&quot;, lin_reg.predict(some_data_prepared)) print(&quot;Labels:&quot;, list(some_labels)) print(100-100*lin_reg.predict(some_data_prepared)/list(some_labels)) 7.2.2 Model Evaluation To measure this regression model performance you can use the mean_squared_error function that performs a mean squared error regression loss: from sklearn.metrics import mean_squared_error df_predictions = lin_reg.predict(df_prepared) lin_mse = mean_squared_error(df_labels, df_predictions, squared = False) # lin_rmse = np.sqrt(lin_mse) # Alternative to (squared = False) lin_mse Mean absolute error: from sklearn.metrics import mean_absolute_error lin_mae = mean_absolute_error(df_labels, df_predictions) lin_mae This result is not very good, mainly because it shows a prediction error of 68,628 dollar (lim_mse). It seems that the model underfit the training data, due to the features that do not provide enough information or the model that is not powerful enough. To fix underfitting, you can select a powerful model or add more features. Let‚Äôs try a Decision Tree Regressor model that is capable of finding complex nonlinear relationships in the data. from sklearn.tree import DecisionTreeRegressor tree_reg = DecisionTreeRegressor(random_state=42) tree_reg.fit(df_prepared, df_labels) df_predictions = tree_reg.predict(df_prepared) tree_mse = mean_squared_error(df_labels, df_predictions) tree_rmse = np.sqrt(tree_mse) tree_rmse #= 0.0 Here the error is 0. The model maybe has overfit the data. To check this you can follow one of the following solutions: Use part of the training set for training and part of it for model validation. Use k-fold cross validation feature. 7.2.2.1 Cross-Validation The cross-validation approach uses different portions of the data to test and train a model on different iterations. You can use the cross_val_score function to randomly splits the training set into k distinct subsets called folds, then it trains and evaluates a model k times, picking a different fold for evaluation every time and training on the other k-1 folds. # k is set to 10 # The result is an array containing the 10 evaluation scores from sklearn.model_selection import cross_val_score scores = cross_val_score(tree_reg, df_prepared, df_labels, scoring=&quot;neg_mean_squared_error&quot;, cv=10) tree_rmse_scores = np.sqrt(-scores) def display_scores(scores): print(&quot;Scores:&quot;, scores) print(&quot;Mean:&quot;, scores.mean()) print(&quot;Standard deviation:&quot;, scores.std()) display_scores(tree_rmse_scores) The Decision Tree has a score of approximately 71,407 with +/- 2,439. Let‚Äôs compute the same scores for Linear Regression. lin_scores = cross_val_score(lin_reg, df_prepared, df_labels,scoring=&quot;neg_mean_squared_error&quot;, cv=10) lin_rmse_scores = np.sqrt(-lin_scores) display_scores(lin_rmse_scores) The Decision Tree model seems to perform worse than the Linear Regression model due to the ovefitting. Let‚Äôs try another model: RandomForestRegressor. Random Forest works by training many Decision Trees on random subsets of the features, than averaging out their predictions. It is an Ensemble Learning model, because it builds a model on top of many other models. from sklearn.ensemble import RandomForestRegressor forest_reg = RandomForestRegressor(n_estimators=100, random_state=42) forest_reg.fit(df_prepared, df_labels) df_predictions = forest_reg.predict(df_prepared) forest_mse = mean_squared_error(df_labels, df_predictions) forest_rmse = np.sqrt(forest_mse) forest_rmse forest_scores = cross_val_score(forest_reg, df_prepared, df_labels,scoring=&quot;neg_mean_squared_error&quot;, cv=10) forest_rmse_scores = np.sqrt(-forest_scores) display_scores(forest_rmse_scores) scores = cross_val_score(lin_reg, df_prepared, df_labels, scoring=&quot;neg_mean_squared_error&quot;, cv=10) pd.Series(np.sqrt(-scores)).describe() Let‚Äôs try another model: Support Vector Machine. Below the SVR (Epsilon-Support Vector Regression) model is used with the kernel=linear parameter. from sklearn.svm import SVR svm_reg = SVR() svm_reg.fit(df_prepared, df_labels) df_predictions = svm_reg.predict(df_prepared) svm_mse = mean_squared_error(df_labels, df_predictions) svm_rmse = np.sqrt(svm_mse) svm_rmse svm_scores = cross_val_score(svm_reg, df_prepared, df_labels,scoring=&quot;neg_mean_squared_error&quot;, cv=10) svm_rmse_scores = np.sqrt(-svm_scores) display_scores(svm_rmse_scores) "],["clustering.html", "Chapter 8 Clustering", " Chapter 8 Clustering Grouping objects into classes, or classification, is a basic cognitive ability and a fundamental scientific methodology and procedure. The definitions had different historical perspectives: + - Hastie et al. (2001): objects in a cluster ‚Äúare more closely related to oneanother than objects assigned to different clusters.‚Äù Han &amp; Kamber (2006): ‚ÄúA cluster is a collection of data objects that aresimilar to one another within the same cluster and are dissimilar to theobjects in other clusters.‚Äù 8.0.1 Clustering The clustering problem: given a data set, divide it into groups such that the homogeneity of each group is maximized or the separation between groups is maximized, or both are maximized. Homogeneity and separation can be formalized in different ways: Similarity or dissimilarity measure A domain-dependent real function of object pairs measuring their similarity or dissimilarity is assumed Similar objects are homogeneous, dissimilar ones are separated Density estimation A statistical estimate of the probability density function that generated the data is computed Homogeneity is high in regions where the estimated p.d.f. is large Frequency and expectation The number of objects in space volumes Homogeneity is high in the collection of space volumes where the number exceeds its expectation 8.0.2 Type of Clustering Models Clustering models can be categorized according to nesting and coverageproperties: Hierarchical or flat Hierarchical: Clusters may be subdivided into smaller, contained sub-clusters, forming a hierarchy. Exclusive, overlapping or fuzzy Exclusive: Any object is an element of exactly one cluster. Overlapping: Any object may be an element of more than one cluster. Fuzzy: For each cluster, a membership function on objects onto [0,1] is defined. Complete or partial Complete: Any object is an element of some cluster Partial: An object may belong to no cluster; objects not belonging to any cluster can be deemed as noise or outliers. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
