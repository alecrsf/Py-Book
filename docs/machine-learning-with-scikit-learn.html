<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Machine Learning with Scikit-Learn | Py-Book</title>
  <meta name="description" content="Welcome to my book about Python" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Machine Learning with Scikit-Learn | Py-Book" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Welcome to my book about Python" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Machine Learning with Scikit-Learn | Py-Book" />
  
  <meta name="twitter:description" content="Welcome to my book about Python" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data-visualization.html"/>
<link rel="next" href="clustering.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/vis-4.16.1/vis.min.css" rel="stylesheet" />
<script src="libs/vis-4.16.1/vis.min.js"></script>
<link href="libs/timeline-0.4/timevis.css" rel="stylesheet" />
<script src="libs/timevis-binding-1.0.0/timevis.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><h2 style:"text-align: center">Py-Book</h2><br> 
<a href="https://alecrsf.netlify.app/en/" style:"text-align: right"> by <b>alecrsf</b></a><li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#setup"><i class="fa fa-check"></i>Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="algorithms.html"><a href="algorithms.html"><i class="fa fa-check"></i><b>1</b> Algorithms</a>
<ul>
<li class="chapter" data-level="1.1" data-path="algorithms.html"><a href="algorithms.html#definitions"><i class="fa fa-check"></i><b>1.1</b> Definitions</a></li>
<li class="chapter" data-level="1.2" data-path="algorithms.html"><a href="algorithms.html#computers-architecture"><i class="fa fa-check"></i><b>1.2</b> Computers Architecture</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>2</b> Basics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="basics.html"><a href="basics.html#control-flow"><i class="fa fa-check"></i><b>2.1</b> Control Flow</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="basics.html"><a href="basics.html#data-types"><i class="fa fa-check"></i><b>2.1.1</b> Data Types</a></li>
<li class="chapter" data-level="2.1.2" data-path="basics.html"><a href="basics.html#variables"><i class="fa fa-check"></i><b>2.1.2</b> Variables</a></li>
<li class="chapter" data-level="2.1.3" data-path="basics.html"><a href="basics.html#logics"><i class="fa fa-check"></i><b>2.1.3</b> Logics</a></li>
<li class="chapter" data-level="2.1.4" data-path="basics.html"><a href="basics.html#iteration"><i class="fa fa-check"></i><b>2.1.4</b> Iteration</a></li>
<li class="chapter" data-level="2.1.5" data-path="basics.html"><a href="basics.html#functions"><i class="fa fa-check"></i><b>2.1.5</b> Functions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="basics.html"><a href="basics.html#collection-types"><i class="fa fa-check"></i><b>2.2</b> Collection Types</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="basics.html"><a href="basics.html#lists"><i class="fa fa-check"></i><b>2.2.1</b> Lists</a></li>
<li class="chapter" data-level="2.2.2" data-path="basics.html"><a href="basics.html#matrixes"><i class="fa fa-check"></i><b>2.2.2</b> Matrixes</a></li>
<li class="chapter" data-level="2.2.3" data-path="basics.html"><a href="basics.html#dictionaries"><i class="fa fa-check"></i><b>2.2.3</b> Dictionaries</a></li>
<li class="chapter" data-level="2.2.4" data-path="basics.html"><a href="basics.html#tuples"><i class="fa fa-check"></i><b>2.2.4</b> Tuples</a></li>
<li class="chapter" data-level="2.2.5" data-path="basics.html"><a href="basics.html#sets"><i class="fa fa-check"></i><b>2.2.5</b> Sets</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="text-mining.html"><a href="text-mining.html"><i class="fa fa-check"></i><b>3</b> Text Mining</a>
<ul>
<li class="chapter" data-level="3.1" data-path="text-mining.html"><a href="text-mining.html#working-with-strings"><i class="fa fa-check"></i><b>3.1</b> Working with strings</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="text-mining.html"><a href="text-mining.html#string-types"><i class="fa fa-check"></i><b>3.1.1</b> String Types</a></li>
<li class="chapter" data-level="3.1.2" data-path="text-mining.html"><a href="text-mining.html#strings-operations"><i class="fa fa-check"></i><b>3.1.2</b> Strings Operations</a></li>
<li class="chapter" data-level="3.1.3" data-path="text-mining.html"><a href="text-mining.html#strings-conversion"><i class="fa fa-check"></i><b>3.1.3</b> Strings Conversion</a></li>
<li class="chapter" data-level="3.1.4" data-path="text-mining.html"><a href="text-mining.html#string-indexing-and-slicing"><i class="fa fa-check"></i><b>3.1.4</b> String Indexing and Slicing</a></li>
<li class="chapter" data-level="3.1.5" data-path="text-mining.html"><a href="text-mining.html#string-splitting-and-joining"><i class="fa fa-check"></i><b>3.1.5</b> String splitting and joining</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="text-mining.html"><a href="text-mining.html#regular-expressions"><i class="fa fa-check"></i><b>3.2</b> Regular Expressions</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="text-mining.html"><a href="text-mining.html#match-find"><i class="fa fa-check"></i><b>3.2.1</b> Match &amp; Find</a></li>
<li class="chapter" data-level="3.2.2" data-path="text-mining.html"><a href="text-mining.html#search-replace"><i class="fa fa-check"></i><b>3.2.2</b> Search &amp; Replace</a></li>
<li class="chapter" data-level="3.2.3" data-path="text-mining.html"><a href="text-mining.html#metacharacters"><i class="fa fa-check"></i><b>3.2.3</b> Metacharacters</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="numpy.html"><a href="numpy.html"><i class="fa fa-check"></i><b>4</b> Numpy</a>
<ul>
<li class="chapter" data-level="4.1" data-path="numpy.html"><a href="numpy.html#arrays"><i class="fa fa-check"></i><b>4.1</b> Arrays</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="numpy.html"><a href="numpy.html#creating-arrays"><i class="fa fa-check"></i><b>4.1.1</b> Creating Arrays</a></li>
<li class="chapter" data-level="4.1.2" data-path="numpy.html"><a href="numpy.html#indexing-and-slicing-arrays"><i class="fa fa-check"></i><b>4.1.2</b> Indexing and slicing arrays</a></li>
<li class="chapter" data-level="4.1.3" data-path="numpy.html"><a href="numpy.html#reshape"><i class="fa fa-check"></i><b>4.1.3</b> Reshape</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="numpy.html"><a href="numpy.html#operators"><i class="fa fa-check"></i><b>4.2</b> Operators</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="numpy.html"><a href="numpy.html#broadcasting-rules"><i class="fa fa-check"></i><b>4.2.1</b> Broadcasting Rules</a></li>
<li class="chapter" data-level="4.2.2" data-path="numpy.html"><a href="numpy.html#upcasting"><i class="fa fa-check"></i><b>4.2.2</b> Upcasting</a></li>
<li class="chapter" data-level="4.2.3" data-path="numpy.html"><a href="numpy.html#conditional-operators"><i class="fa fa-check"></i><b>4.2.3</b> Conditional operators</a></li>
<li class="chapter" data-level="4.2.4" data-path="numpy.html"><a href="numpy.html#statistics"><i class="fa fa-check"></i><b>4.2.4</b> Statistics</a></li>
<li class="chapter" data-level="4.2.5" data-path="numpy.html"><a href="numpy.html#linear-algebra"><i class="fa fa-check"></i><b>4.2.5</b> Linear Algebra</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="pandas.html"><a href="pandas.html"><i class="fa fa-check"></i><b>5</b> Pandas</a>
<ul>
<li class="chapter" data-level="5.1" data-path="pandas.html"><a href="pandas.html#series"><i class="fa fa-check"></i><b>5.1</b> Series</a></li>
<li class="chapter" data-level="5.2" data-path="pandas.html"><a href="pandas.html#dataframe"><i class="fa fa-check"></i><b>5.2</b> DataFrame</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="pandas.html"><a href="pandas.html#indexing"><i class="fa fa-check"></i><b>5.2.1</b> Indexing</a></li>
<li class="chapter" data-level="5.2.2" data-path="pandas.html"><a href="pandas.html#reading-data"><i class="fa fa-check"></i><b>5.2.2</b> Reading Data</a></li>
<li class="chapter" data-level="5.2.3" data-path="pandas.html"><a href="pandas.html#data-wrangling"><i class="fa fa-check"></i><b>5.2.3</b> Data Wrangling</a></li>
<li class="chapter" data-level="5.2.4" data-path="pandas.html"><a href="pandas.html#evaluating-an-expression"><i class="fa fa-check"></i><b>5.2.4</b> Evaluating an expression</a></li>
<li class="chapter" data-level="5.2.5" data-path="pandas.html"><a href="pandas.html#handling-missing-data"><i class="fa fa-check"></i><b>5.2.5</b> Handling Missing Data</a></li>
<li class="chapter" data-level="5.2.6" data-path="pandas.html"><a href="pandas.html#data-manipulation"><i class="fa fa-check"></i><b>5.2.6</b> Data Manipulation</a></li>
<li class="chapter" data-level="5.2.7" data-path="pandas.html"><a href="pandas.html#overview-functions"><i class="fa fa-check"></i><b>5.2.7</b> Overview Functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data-visualization.html"><a href="data-visualization.html"><i class="fa fa-check"></i><b>6</b> Data Visualization</a>
<ul>
<li class="chapter" data-level="6.1" data-path="data-visualization.html"><a href="data-visualization.html#matplotlib"><i class="fa fa-check"></i><b>6.1</b> Matplotlib</a></li>
<li class="chapter" data-level="6.2" data-path="data-visualization.html"><a href="data-visualization.html#exploratory-visualization"><i class="fa fa-check"></i><b>6.2</b> Exploratory Visualization</a></li>
<li class="chapter" data-level="6.3" data-path="data-visualization.html"><a href="data-visualization.html#other-packages"><i class="fa fa-check"></i><b>6.3</b> Other Packages</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="data-visualization.html"><a href="data-visualization.html#seaborn"><i class="fa fa-check"></i><b>6.3.1</b> Seaborn</a></li>
<li class="chapter" data-level="6.3.2" data-path="data-visualization.html"><a href="data-visualization.html#bokeh"><i class="fa fa-check"></i><b>6.3.2</b> Bokeh</a></li>
<li class="chapter" data-level="6.3.3" data-path="data-visualization.html"><a href="data-visualization.html#plotly"><i class="fa fa-check"></i><b>6.3.3</b> Plotly</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="machine-learning-with-scikit-learn.html"><a href="machine-learning-with-scikit-learn.html"><i class="fa fa-check"></i><b>7</b> Machine Learning with <strong>Scikit-Learn</strong></a>
<ul>
<li class="chapter" data-level="7.1" data-path="machine-learning-with-scikit-learn.html"><a href="machine-learning-with-scikit-learn.html#data-preprocessing"><i class="fa fa-check"></i><b>7.1</b> Data PreProcessing</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="machine-learning-with-scikit-learn.html"><a href="machine-learning-with-scikit-learn.html#create-a-training-and-test-sets"><i class="fa fa-check"></i><b>7.1.1</b> Create a Training and Test Sets</a></li>
<li class="chapter" data-level="7.1.2" data-path="machine-learning-with-scikit-learn.html"><a href="machine-learning-with-scikit-learn.html#impute-missing-values"><i class="fa fa-check"></i><b>7.1.2</b> Impute Missing Values</a></li>
<li class="chapter" data-level="7.1.3" data-path="machine-learning-with-scikit-learn.html"><a href="machine-learning-with-scikit-learn.html#handling-texts-and-categorical-attributes"><i class="fa fa-check"></i><b>7.1.3</b> Handling Texts and Categorical Attributes</a></li>
<li class="chapter" data-level="7.1.4" data-path="machine-learning-with-scikit-learn.html"><a href="machine-learning-with-scikit-learn.html#feature-scaling"><i class="fa fa-check"></i><b>7.1.4</b> Feature Scaling</a></li>
<li class="chapter" data-level="7.1.5" data-path="machine-learning-with-scikit-learn.html"><a href="machine-learning-with-scikit-learn.html#pipelines"><i class="fa fa-check"></i><b>7.1.5</b> Pipelines</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="machine-learning-with-scikit-learn.html"><a href="machine-learning-with-scikit-learn.html#select-and-train-a-model"><i class="fa fa-check"></i><b>7.2</b> Select and Train a Model</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="machine-learning-with-scikit-learn.html"><a href="machine-learning-with-scikit-learn.html#training-and-evaluating-on-the-training-set"><i class="fa fa-check"></i><b>7.2.1</b> Training and Evaluating on the Training Set</a></li>
<li class="chapter" data-level="7.2.2" data-path="machine-learning-with-scikit-learn.html"><a href="machine-learning-with-scikit-learn.html#model-evaluation"><i class="fa fa-check"></i><b>7.2.2</b> Model Evaluation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>8</b> Clustering</a>
<ul>
<li class="chapter" data-level="8.0.1" data-path="clustering.html"><a href="clustering.html#clustering-1"><i class="fa fa-check"></i><b>8.0.1</b> Clustering</a></li>
<li class="chapter" data-level="8.0.2" data-path="clustering.html"><a href="clustering.html#type-of-clustering-models"><i class="fa fa-check"></i><b>8.0.2</b> Type of Clustering Models</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://rstudio.com"> <img src="chapter/html/rstudio.png" width=20%></a></li> 

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Py-Book</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machine-learning-with-scikit-learn" class="section level1" number="7">
<h1><span class="header-section-number">Chapter 7</span> Machine Learning with <strong>Scikit-Learn</strong></h1>
<iframe src="https://scikit-learn.org/dev/user_guide.html" height="500px" width="100%">
</iframe>
<div id="data-preprocessing" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Data PreProcessing</h2>
<div id="create-a-training-and-test-sets" class="section level3" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Create a Training and Test Sets</h3>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"><code>train_test_split()</code></a>, this function randomly splits the dataset in training and test sets. <code>random_state</code> allows you to set the random generator seed and to make this notebook’s output identical at every run.</li>
</ul>
<div class="sourceCode" id="cb137"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb137-1"><a href="machine-learning-with-scikit-learn.html#cb137-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb137-2"><a href="machine-learning-with-scikit-learn.html#cb137-2"></a></span>
<span id="cb137-3"><a href="machine-learning-with-scikit-learn.html#cb137-3"></a>train_set, test_set <span class="op">=</span> train_test_split(df, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span></code></pre></div>
<ul>
<li><code>StratifiedShuffleSplit()</code>, this algorithm provides train and test indexes to split (<strong>stratify</strong>) data in train and test sets. For example based on a <em>column</em> of my dataset <em>df</em></li>
</ul>
<div class="sourceCode" id="cb138"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb138-1"><a href="machine-learning-with-scikit-learn.html#cb138-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> StratifiedShuffleSplit</span>
<span id="cb138-2"><a href="machine-learning-with-scikit-learn.html#cb138-2"></a></span>
<span id="cb138-3"><a href="machine-learning-with-scikit-learn.html#cb138-3"></a>split <span class="op">=</span> StratifiedShuffleSplit(n_splits<span class="op">=</span><span class="dv">1</span>, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb138-4"><a href="machine-learning-with-scikit-learn.html#cb138-4"></a></span>
<span id="cb138-5"><a href="machine-learning-with-scikit-learn.html#cb138-5"></a><span class="cf">for</span> train_index, test_index <span class="kw">in</span> split.split(df, df[<span class="st">&quot;column&quot;</span>]):</span>
<span id="cb138-6"><a href="machine-learning-with-scikit-learn.html#cb138-6"></a>    strat_train_set <span class="op">=</span> df.loc[train_index]</span>
<span id="cb138-7"><a href="machine-learning-with-scikit-learn.html#cb138-7"></a>    strat_test_set <span class="op">=</span> df.loc[test_index]</span></code></pre></div>
<p>Is it possible to compare the different proportions based on the split, in the overall dataset, the test set generated with the stratified sampling, in a test set generated using random sampling</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb139-1"><a href="machine-learning-with-scikit-learn.html#cb139-1"></a><span class="kw">def</span> income_cat_proportions(data):</span>
<span id="cb139-2"><a href="machine-learning-with-scikit-learn.html#cb139-2"></a>    <span class="cf">return</span> data[<span class="st">&quot;column&quot;</span>].value_counts() <span class="op">/</span> <span class="bu">len</span>(data)</span>
<span id="cb139-3"><a href="machine-learning-with-scikit-learn.html#cb139-3"></a></span>
<span id="cb139-4"><a href="machine-learning-with-scikit-learn.html#cb139-4"></a>train_set, test_set <span class="op">=</span> train_test_split(df, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb139-5"><a href="machine-learning-with-scikit-learn.html#cb139-5"></a></span>
<span id="cb139-6"><a href="machine-learning-with-scikit-learn.html#cb139-6"></a>compare_props <span class="op">=</span> pd.DataFrame({</span>
<span id="cb139-7"><a href="machine-learning-with-scikit-learn.html#cb139-7"></a>    <span class="st">&quot;Overall&quot;</span>: income_cat_proportions(df),</span>
<span id="cb139-8"><a href="machine-learning-with-scikit-learn.html#cb139-8"></a>    <span class="st">&quot;Stratified&quot;</span>: income_cat_proportions(strat_test_set),</span>
<span id="cb139-9"><a href="machine-learning-with-scikit-learn.html#cb139-9"></a>    <span class="st">&quot;Random&quot;</span>: income_cat_proportions(test_set)}</span>
<span id="cb139-10"><a href="machine-learning-with-scikit-learn.html#cb139-10"></a>    ).sort_index()</span>
<span id="cb139-11"><a href="machine-learning-with-scikit-learn.html#cb139-11"></a></span>
<span id="cb139-12"><a href="machine-learning-with-scikit-learn.html#cb139-12"></a>compare_props[<span class="st">&quot;Rand. </span><span class="sc">%E</span><span class="st">rror&quot;</span>] <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> compare_props[<span class="st">&quot;Random&quot;</span>] <span class="op">/</span> compare_props[<span class="st">&quot;Overall&quot;</span>] <span class="op">-</span> <span class="dv">100</span></span>
<span id="cb139-13"><a href="machine-learning-with-scikit-learn.html#cb139-13"></a>compare_props[<span class="st">&quot;Strat. </span><span class="sc">%E</span><span class="st">rror&quot;</span>] <span class="op">=</span> <span class="dv">100</span> <span class="op">*</span> compare_props[<span class="st">&quot;Stratified&quot;</span>] <span class="op">/</span> compare_props[<span class="st">&quot;Overall&quot;</span>] <span class="op">-</span> <span class="dv">100</span></span>
<span id="cb139-14"><a href="machine-learning-with-scikit-learn.html#cb139-14"></a></span>
<span id="cb139-15"><a href="machine-learning-with-scikit-learn.html#cb139-15"></a>compare_props</span></code></pre></div>
<p>The test set generated using stratified sampling has category proportions almost identical to those in the full dataset. Remove the income_cat.</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb140-1"><a href="machine-learning-with-scikit-learn.html#cb140-1"></a><span class="cf">for</span> set_ <span class="kw">in</span> (strat_train_set, strat_test_set):</span>
<span id="cb140-2"><a href="machine-learning-with-scikit-learn.html#cb140-2"></a>    set_.drop(<span class="st">&quot;income_cat&quot;</span>, axis<span class="op">=</span><span class="dv">1</span>, inplace<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>After choosing the best technique to split the dataset (for example, in this case the stratified sampling), redefine your <em>df</em></p>
<p>Remember to separate the <strong>predictors</strong> (i.e median_house_value) by the labels</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb141-1"><a href="machine-learning-with-scikit-learn.html#cb141-1"></a><span class="co"># drop labels for training set</span></span>
<span id="cb141-2"><a href="machine-learning-with-scikit-learn.html#cb141-2"></a>df <span class="op">=</span> strat_train_set.drop(<span class="st">&quot;median_house_value&quot;</span>, axis<span class="op">=</span><span class="dv">1</span>) </span>
<span id="cb141-3"><a href="machine-learning-with-scikit-learn.html#cb141-3"></a></span>
<span id="cb141-4"><a href="machine-learning-with-scikit-learn.html#cb141-4"></a>df_labels <span class="op">=</span> strat_train_set[<span class="st">&quot;median_house_value&quot;</span>].copy()</span></code></pre></div>
</div>
<div id="impute-missing-values" class="section level3" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Impute Missing Values</h3>
<p>Machine Learning algorithms do not work with missing values.</p>
<p>Use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html"><code>SimpleImputer()</code></a> function to replace <code>na</code> values with the median values. The strategy parameter allows you to specify the imputation strategy.</p>
<p>Remember also that imputation techniques doesn’t apply to categorical variables, so make sure you remove them from your df.</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb142-1"><a href="machine-learning-with-scikit-learn.html#cb142-1"></a><span class="im">from</span> sklearn.impute <span class="im">import</span> SimpleImputer</span>
<span id="cb142-2"><a href="machine-learning-with-scikit-learn.html#cb142-2"></a></span>
<span id="cb142-3"><a href="machine-learning-with-scikit-learn.html#cb142-3"></a>imputer <span class="op">=</span> SimpleImputer(strategy<span class="op">=</span><span class="st">&quot;median&quot;</span>)</span>
<span id="cb142-4"><a href="machine-learning-with-scikit-learn.html#cb142-4"></a>imputer.strategy <span class="co"># &#39;median&#39;</span></span>
<span id="cb142-5"><a href="machine-learning-with-scikit-learn.html#cb142-5"></a></span>
<span id="cb142-6"><a href="machine-learning-with-scikit-learn.html#cb142-6"></a><span class="co">#You can fit the imputer instance to the training data with the fit() function</span></span>
<span id="cb142-7"><a href="machine-learning-with-scikit-learn.html#cb142-7"></a>imputer.fit(df)</span>
<span id="cb142-8"><a href="machine-learning-with-scikit-learn.html#cb142-8"></a></span>
<span id="cb142-9"><a href="machine-learning-with-scikit-learn.html#cb142-9"></a><span class="co">#Imputer computes the median to the numerical attributes and stores results in statistics_ variable, which is equal to df.median().values:</span></span>
<span id="cb142-10"><a href="machine-learning-with-scikit-learn.html#cb142-10"></a>imputer.statistics_ </span>
<span id="cb142-11"><a href="machine-learning-with-scikit-learn.html#cb142-11"></a></span>
<span id="cb142-12"><a href="machine-learning-with-scikit-learn.html#cb142-12"></a><span class="co">#At this point you can define the training set by replacing the missing values with the median values of the learned medians. The result (X) is a numpy array containing the transformed features. You get the same results by applying fit_transform()</span></span>
<span id="cb142-13"><a href="machine-learning-with-scikit-learn.html#cb142-13"></a>X <span class="op">=</span> imputer.transform(df)</span>
<span id="cb142-14"><a href="machine-learning-with-scikit-learn.html#cb142-14"></a></span>
<span id="cb142-15"><a href="machine-learning-with-scikit-learn.html#cb142-15"></a>df_tr <span class="op">=</span> pd.DataFrame(X, columns<span class="op">=</span> df.columns,</span>
<span id="cb142-16"><a href="machine-learning-with-scikit-learn.html#cb142-16"></a>                          index<span class="op">=</span> df.index)</span></code></pre></div>
</div>
<div id="handling-texts-and-categorical-attributes" class="section level3" number="7.1.3">
<h3><span class="header-section-number">7.1.3</span> Handling Texts and Categorical Attributes</h3>
<p>Now let’s preprocess the categorical input feature. We need to convert these categories from text to numbers. Let’s define an arbitrary dataframe containing the values of our categorical column <code>df_cat = df[["categorical_column"]]</code></p>
<div id="ordinal-encoder" class="section level4" number="7.1.3.1">
<h4><span class="header-section-number">7.1.3.1</span> Ordinal Encoder</h4>
<p>Try <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html"><code>OrdinalEncoder()</code></a> function that encodes categorical features as an integer array. By using the <strong><code>OrdinalEncoder</code></strong> approach <em>ML</em> techniques will assume that two nearby values are more similar than two distant values.</p>
<p>Be careful as it may not be the case of your study; for example, if categories were encoded like this <code>0=Low-Income</code> and <code>4=Low/Mid-Income</code>, notice that these are more similar than <code>0=Low-Income</code> and <code>1=High-Income</code></p>
<div class="sourceCode" id="cb143"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb143-1"><a href="machine-learning-with-scikit-learn.html#cb143-1"></a>df_cat <span class="op">=</span> df[[<span class="st">&quot;categorical_column&quot;</span>]]</span>
<span id="cb143-2"><a href="machine-learning-with-scikit-learn.html#cb143-2"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OrdinalEncoder</span>
<span id="cb143-3"><a href="machine-learning-with-scikit-learn.html#cb143-3"></a></span>
<span id="cb143-4"><a href="machine-learning-with-scikit-learn.html#cb143-4"></a>ordinal_encoder <span class="op">=</span> OrdinalEncoder()</span>
<span id="cb143-5"><a href="machine-learning-with-scikit-learn.html#cb143-5"></a>df_cat_encoded <span class="op">=</span> ordinal_encoder.fit_transform(df_cat)</span>
<span id="cb143-6"><a href="machine-learning-with-scikit-learn.html#cb143-6"></a>df_cat_encoded[:<span class="dv">10</span>] <span class="co">#to see the df with encoded variables</span></span>
<span id="cb143-7"><a href="machine-learning-with-scikit-learn.html#cb143-7"></a></span>
<span id="cb143-8"><a href="machine-learning-with-scikit-learn.html#cb143-8"></a>np.unique(df_cat_encoded) <span class="co">#to see the encodings</span></span>
<span id="cb143-9"><a href="machine-learning-with-scikit-learn.html#cb143-9"></a></span>
<span id="cb143-10"><a href="machine-learning-with-scikit-learn.html#cb143-10"></a><span class="co">#The ordinal_encoder stores the list of categories in its categories_ variable. It is a list of 1D array of categories for each categorical variable: (in this case we have just 1 categorical attribute):</span></span>
<span id="cb143-11"><a href="machine-learning-with-scikit-learn.html#cb143-11"></a>ordinal_encoder.categories_</span></code></pre></div>
</div>
<div id="one-hot-encoder" class="section level4" number="7.1.3.2">
<h4><span class="header-section-number">7.1.3.2</span> One Hot Encoder</h4>
<p>The <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"><code>OneHotEncoder()</code></a> function encodes categorical features as a one-hot numeric array. It allows you to create a binary attribute per category. For example:</p>
<ul>
<li>one attribute is equal to 1 when category is <code>Low-Income</code>, 0 otherwise;</li>
<li>one attribute is equal to 1 when category is <code>Mid-Income</code>, 0 otherwise;</li>
<li>and so on</li>
</ul>
<p>This approach is called <code>one-hot encoding</code> because, for the corresponding row, only one attribute will be equal to 1, while the others will be 0.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb144-1"><a href="machine-learning-with-scikit-learn.html#cb144-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder</span>
<span id="cb144-2"><a href="machine-learning-with-scikit-learn.html#cb144-2"></a></span>
<span id="cb144-3"><a href="machine-learning-with-scikit-learn.html#cb144-3"></a>1hot_encoder <span class="op">=</span> OneHotEncoder()</span>
<span id="cb144-4"><a href="machine-learning-with-scikit-learn.html#cb144-4"></a>df_cat_1hot <span class="op">=</span> 1hot_encoder.fit_transform(df_cat)</span>
<span id="cb144-5"><a href="machine-learning-with-scikit-learn.html#cb144-5"></a>df_cat_1hot</span></code></pre></div>
<p>By default, the <code>OneHotEncoder()</code> returns a sparse matrix. Each row is full of 0s except for a single 1 per row. This can consume tons of memory, you can only store the location of the nonzero elements. You can convert the sparse matrix to a dense array, if needed, by calling the <code>toarray()</code>function.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb145-1"><a href="machine-learning-with-scikit-learn.html#cb145-1"></a>df_cat_1hot.toarray()</span>
<span id="cb145-2"><a href="machine-learning-with-scikit-learn.html#cb145-2"></a></span>
<span id="cb145-3"><a href="machine-learning-with-scikit-learn.html#cb145-3"></a><span class="co"># Alternatively, you can set sparse=False when creating the OneHotEncoder:</span></span>
<span id="cb145-4"><a href="machine-learning-with-scikit-learn.html#cb145-4"></a>1hot_encoder <span class="op">=</span> OneHotEncoder(sparse<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb145-5"><a href="machine-learning-with-scikit-learn.html#cb145-5"></a>df_cat_1hot <span class="op">=</span> 1hot_encoder.fit_transform(df_cat)</span>
<span id="cb145-6"><a href="machine-learning-with-scikit-learn.html#cb145-6"></a>df_cat_1hot</span>
<span id="cb145-7"><a href="machine-learning-with-scikit-learn.html#cb145-7"></a></span>
<span id="cb145-8"><a href="machine-learning-with-scikit-learn.html#cb145-8"></a><span class="co"># You can list the encoder&#39;s categories_ variable.</span></span>
<span id="cb145-9"><a href="machine-learning-with-scikit-learn.html#cb145-9"></a>cat_encoder.categories_</span></code></pre></div>
</div>
</div>
<div id="feature-scaling" class="section level3" number="7.1.4">
<h3><span class="header-section-number">7.1.4</span> Feature Scaling</h3>
<p>Some Machine Learning techniques do not perform well when the input have different scales. Two simple ways to get all attributes with the same scale are:</p>
<ul>
<li><p><code>min-max scaling or normalization</code>: all values are shifted and rescaled so that they end up ranging from 0 to 1. First subtract the min value and then divide by the max minus the min. This approach is supported by <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"><code>MinMaxScaler()</code></a> class.</p></li>
<li><p><code>standardization</code>: first it subtracts the mean value and then it divides by the standard deviation. This approach is supported by <a href="https://www.google.com/search?q=standard+scaler&amp;oq=standard+scaler&amp;aqs=chrome..69i57j0i10i512j0i20i263i512j0i512l7.2497j0j4&amp;sourceid=chrome&amp;ie=UTF-8"><code>StandardScaler()</code></a> class.</p></li>
</ul>
<p>You need to apply this procedure to the training data only.</p>
</div>
<div id="pipelines" class="section level3" number="7.1.5">
<h3><span class="header-section-number">7.1.5</span> Pipelines</h3>
<p>The <a href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"><code>Pipeline</code></a> class helps with the data cleaning steps. Basically we can define the workflow that we’ve applied previously , within few simple lines of code.</p>
<p>Let’s build a pipeline for preprocessing the <strong>numerical attributes</strong> of the dataframe first:</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb146-1"><a href="machine-learning-with-scikit-learn.html#cb146-1"></a>df_num <span class="op">=</span> df.drop(<span class="st">&quot;categorical_variable_if_any&quot;</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb146-2"><a href="machine-learning-with-scikit-learn.html#cb146-2"></a></span>
<span id="cb146-3"><a href="machine-learning-with-scikit-learn.html#cb146-3"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb146-4"><a href="machine-learning-with-scikit-learn.html#cb146-4"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb146-5"><a href="machine-learning-with-scikit-learn.html#cb146-5"></a></span>
<span id="cb146-6"><a href="machine-learning-with-scikit-learn.html#cb146-6"></a>num_pipeline <span class="op">=</span> Pipeline([</span>
<span id="cb146-7"><a href="machine-learning-with-scikit-learn.html#cb146-7"></a>        (<span class="st">&#39;imputer&#39;</span>, SimpleImputer(strategy<span class="op">=</span><span class="st">&quot;median&quot;</span>)),</span>
<span id="cb146-8"><a href="machine-learning-with-scikit-learn.html#cb146-8"></a>        (<span class="st">&#39;attribs_adder&#39;</span>, CombinedAttributesAdder()),</span>
<span id="cb146-9"><a href="machine-learning-with-scikit-learn.html#cb146-9"></a>        (<span class="st">&#39;std_scaler&#39;</span>, StandardScaler())</span>
<span id="cb146-10"><a href="machine-learning-with-scikit-learn.html#cb146-10"></a>        ])</span>
<span id="cb146-11"><a href="machine-learning-with-scikit-learn.html#cb146-11"></a></span>
<span id="cb146-12"><a href="machine-learning-with-scikit-learn.html#cb146-12"></a>df_num_tr <span class="op">=</span> num_pipeline.fit_transform(df_num)</span></code></pre></div>
<p>The <code>Pipeline</code> class takes a list of <code>name,estimator</code> pairs defining a sequence steps.</p>
<ul>
<li><p>The <code>names</code> must be unique and do not contain <code>__</code>.</p></li>
<li><p>The <code>estimators</code> are <code>transformers</code>.</p></li>
</ul>
<p>The <code>num_pipleline</code> object calls <code>fit_transform()</code> sequentially on all transformers, passing the output of each call as the parameter to the next call until it reaches the final estimator.</p>
<p>Now let’s build a pipeline for <strong>preprocessing both the numerical attributes and categorical attributes</strong>. Use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html"><code>ColumnTransformer</code></a> class that applies transformers to columns of an array or pandas DataFrame.</p>
<p>The <code>ColumnTransformer()</code> function requires a list of tuples, where each tuple contains a name, a transformer and a list of names (or indexes) of columns that the transformer should be applied to.</p>
<p>The <code>full_pipeline</code> object applies a <code>fit_transform()</code> to the df data.</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb147-1"><a href="machine-learning-with-scikit-learn.html#cb147-1"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> ColumnTransformer</span>
<span id="cb147-2"><a href="machine-learning-with-scikit-learn.html#cb147-2"></a></span>
<span id="cb147-3"><a href="machine-learning-with-scikit-learn.html#cb147-3"></a>num_attribs <span class="op">=</span> <span class="bu">list</span>(df_num)</span>
<span id="cb147-4"><a href="machine-learning-with-scikit-learn.html#cb147-4"></a>cat_attribs <span class="op">=</span> [<span class="st">&quot;categorical_variable_if_any&quot;</span>]</span>
<span id="cb147-5"><a href="machine-learning-with-scikit-learn.html#cb147-5"></a></span>
<span id="cb147-6"><a href="machine-learning-with-scikit-learn.html#cb147-6"></a>full_pipeline <span class="op">=</span> ColumnTransformer([</span>
<span id="cb147-7"><a href="machine-learning-with-scikit-learn.html#cb147-7"></a>    <span class="co">#numerical attributes transformed with num_pipeline</span></span>
<span id="cb147-8"><a href="machine-learning-with-scikit-learn.html#cb147-8"></a>        (<span class="st">&quot;num&quot;</span>, num_pipeline, num_attribs), </span>
<span id="cb147-9"><a href="machine-learning-with-scikit-learn.html#cb147-9"></a>  <span class="co">#categorical variables transformed with onehotencoder</span></span>
<span id="cb147-10"><a href="machine-learning-with-scikit-learn.html#cb147-10"></a>        (<span class="st">&quot;cat&quot;</span>, OneHotEncoder(), cat_attribs), </span>
<span id="cb147-11"><a href="machine-learning-with-scikit-learn.html#cb147-11"></a>    ])</span>
<span id="cb147-12"><a href="machine-learning-with-scikit-learn.html#cb147-12"></a></span>
<span id="cb147-13"><a href="machine-learning-with-scikit-learn.html#cb147-13"></a>df_prepared <span class="op">=</span> full_pipeline.fit_transform(df)</span></code></pre></div>
</div>
</div>
<div id="select-and-train-a-model" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Select and Train a Model</h2>
<p>The flowchart below is designed to give users a bit of a rough guide on how to approach problems with regard to which estimators to try on your data</p>
<p><a href="https://scikit-learn.org/dev/tutorial/machine_learning_map/index.html"><img src="images/ml_map.png" title="Choosing the right estimator" /></a></p>
<div id="training-and-evaluating-on-the-training-set" class="section level3" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Training and Evaluating on the Training Set</h3>
<p>Let’s first train a Linear Regression model</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb148-1"><a href="machine-learning-with-scikit-learn.html#cb148-1"></a>df <span class="op">=</span> strat_train_set.drop(<span class="st">&quot;median_house_value&quot;</span>, axis<span class="op">=</span><span class="dv">1</span>) <span class="co"># drop labels for training set</span></span>
<span id="cb148-2"><a href="machine-learning-with-scikit-learn.html#cb148-2"></a>df_labels <span class="op">=</span> strat_train_set[<span class="st">&quot;median_house_value&quot;</span>].copy()</span></code></pre></div>
<div class="sourceCode" id="cb149"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb149-1"><a href="machine-learning-with-scikit-learn.html#cb149-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb149-2"><a href="machine-learning-with-scikit-learn.html#cb149-2"></a></span>
<span id="cb149-3"><a href="machine-learning-with-scikit-learn.html#cb149-3"></a>lin_reg <span class="op">=</span> LinearRegression()</span>
<span id="cb149-4"><a href="machine-learning-with-scikit-learn.html#cb149-4"></a>lin_reg.fit(df_prepared, df_labels)</span></code></pre></div>
<p>Let’s try the full preprocessing pipeline on a few training instances: the first 5 rows:</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb150-1"><a href="machine-learning-with-scikit-learn.html#cb150-1"></a>some_data <span class="op">=</span> df.iloc[:<span class="dv">5</span>]</span>
<span id="cb150-2"><a href="machine-learning-with-scikit-learn.html#cb150-2"></a>some_labels <span class="op">=</span> df_labels.iloc[:<span class="dv">5</span>]</span>
<span id="cb150-3"><a href="machine-learning-with-scikit-learn.html#cb150-3"></a>some_data_prepared <span class="op">=</span> full_pipeline.transform(some_data)</span>
<span id="cb150-4"><a href="machine-learning-with-scikit-learn.html#cb150-4"></a></span>
<span id="cb150-5"><a href="machine-learning-with-scikit-learn.html#cb150-5"></a><span class="bu">print</span>(<span class="st">&#39;Compare predictions against the actual values.&#39;</span>)</span>
<span id="cb150-6"><a href="machine-learning-with-scikit-learn.html#cb150-6"></a><span class="bu">print</span>(<span class="st">&quot;Predictions:&quot;</span>, lin_reg.predict(some_data_prepared))</span>
<span id="cb150-7"><a href="machine-learning-with-scikit-learn.html#cb150-7"></a><span class="bu">print</span>(<span class="st">&quot;Labels:&quot;</span>, <span class="bu">list</span>(some_labels)) </span>
<span id="cb150-8"><a href="machine-learning-with-scikit-learn.html#cb150-8"></a></span>
<span id="cb150-9"><a href="machine-learning-with-scikit-learn.html#cb150-9"></a><span class="bu">print</span>(<span class="dv">100</span><span class="op">-</span><span class="dv">100</span><span class="op">*</span>lin_reg.predict(some_data_prepared)<span class="op">/</span><span class="bu">list</span>(some_labels))</span></code></pre></div>
</div>
<div id="model-evaluation" class="section level3" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Model Evaluation</h3>
<p>To measure this regression model performance you can use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html"><code>mean_squared_error</code></a> function that performs a mean squared error regression loss:</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb151-1"><a href="machine-learning-with-scikit-learn.html#cb151-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb151-2"><a href="machine-learning-with-scikit-learn.html#cb151-2"></a></span>
<span id="cb151-3"><a href="machine-learning-with-scikit-learn.html#cb151-3"></a>df_predictions <span class="op">=</span> lin_reg.predict(df_prepared)</span>
<span id="cb151-4"><a href="machine-learning-with-scikit-learn.html#cb151-4"></a></span>
<span id="cb151-5"><a href="machine-learning-with-scikit-learn.html#cb151-5"></a>lin_mse <span class="op">=</span> mean_squared_error(df_labels, df_predictions, squared <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb151-6"><a href="machine-learning-with-scikit-learn.html#cb151-6"></a><span class="co"># lin_rmse = np.sqrt(lin_mse) # Alternative to (squared = False)</span></span>
<span id="cb151-7"><a href="machine-learning-with-scikit-learn.html#cb151-7"></a></span>
<span id="cb151-8"><a href="machine-learning-with-scikit-learn.html#cb151-8"></a>lin_mse</span></code></pre></div>
<p>Mean absolute error:</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb152-1"><a href="machine-learning-with-scikit-learn.html#cb152-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_absolute_error</span>
<span id="cb152-2"><a href="machine-learning-with-scikit-learn.html#cb152-2"></a></span>
<span id="cb152-3"><a href="machine-learning-with-scikit-learn.html#cb152-3"></a>lin_mae <span class="op">=</span> mean_absolute_error(df_labels, df_predictions)</span>
<span id="cb152-4"><a href="machine-learning-with-scikit-learn.html#cb152-4"></a>lin_mae</span></code></pre></div>
<p>This result is not very good, mainly because it shows a prediction error of 68,628 dollar (lim_mse). It seems that the model underfit the training data, due to the features that do not provide enough information or the model that is not powerful enough. To fix underfitting, you can select a powerful model or add more features.</p>
<p>Let’s try a Decision Tree Regressor model that is capable of finding complex nonlinear relationships in the data.</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb153-1"><a href="machine-learning-with-scikit-learn.html#cb153-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb153-2"><a href="machine-learning-with-scikit-learn.html#cb153-2"></a></span>
<span id="cb153-3"><a href="machine-learning-with-scikit-learn.html#cb153-3"></a>tree_reg <span class="op">=</span> DecisionTreeRegressor(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb153-4"><a href="machine-learning-with-scikit-learn.html#cb153-4"></a>tree_reg.fit(df_prepared, df_labels) </span>
<span id="cb153-5"><a href="machine-learning-with-scikit-learn.html#cb153-5"></a></span>
<span id="cb153-6"><a href="machine-learning-with-scikit-learn.html#cb153-6"></a>df_predictions <span class="op">=</span> tree_reg.predict(df_prepared)</span>
<span id="cb153-7"><a href="machine-learning-with-scikit-learn.html#cb153-7"></a>tree_mse <span class="op">=</span> mean_squared_error(df_labels, df_predictions)</span>
<span id="cb153-8"><a href="machine-learning-with-scikit-learn.html#cb153-8"></a>tree_rmse <span class="op">=</span> np.sqrt(tree_mse)</span>
<span id="cb153-9"><a href="machine-learning-with-scikit-learn.html#cb153-9"></a></span>
<span id="cb153-10"><a href="machine-learning-with-scikit-learn.html#cb153-10"></a>tree_rmse <span class="co">#= 0.0</span></span></code></pre></div>
<p>Here the error is 0. The model maybe has overfit the data. To check this you can follow one of the following solutions:</p>
<ul>
<li>Use part of the training set for training and part of it for model validation.</li>
<li>Use <strong>k-fold cross validation</strong> feature.</li>
</ul>
<div id="cross-validation" class="section level4" number="7.2.2.1">
<h4><span class="header-section-number">7.2.2.1</span> Cross-Validation</h4>
<p>The <a href="https://scikit-learn.org/stable/modules/cross_validation.html"><code>cross-validation</code></a> approach uses different portions of the data to test and train a model on different iterations. You can use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html"><code>cross_val_score</code></a> function to randomly splits the training set into <code>k</code> distinct subsets called <code>folds</code>, then it trains and evaluates a model <code>k</code> times, picking a different <code>fold</code> for evaluation every time and training on the other <code>k-1</code> folds.</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb154-1"><a href="machine-learning-with-scikit-learn.html#cb154-1"></a><span class="co"># k is set to 10</span></span>
<span id="cb154-2"><a href="machine-learning-with-scikit-learn.html#cb154-2"></a><span class="co"># The result is an array containing the 10 evaluation scores</span></span>
<span id="cb154-3"><a href="machine-learning-with-scikit-learn.html#cb154-3"></a></span>
<span id="cb154-4"><a href="machine-learning-with-scikit-learn.html#cb154-4"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb154-5"><a href="machine-learning-with-scikit-learn.html#cb154-5"></a></span>
<span id="cb154-6"><a href="machine-learning-with-scikit-learn.html#cb154-6"></a>scores <span class="op">=</span> cross_val_score(tree_reg, df_prepared,</span>
<span id="cb154-7"><a href="machine-learning-with-scikit-learn.html#cb154-7"></a>df_labels, scoring<span class="op">=</span><span class="st">&quot;neg_mean_squared_error&quot;</span>, cv<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb154-8"><a href="machine-learning-with-scikit-learn.html#cb154-8"></a></span>
<span id="cb154-9"><a href="machine-learning-with-scikit-learn.html#cb154-9"></a>tree_rmse_scores <span class="op">=</span> np.sqrt(<span class="op">-</span>scores)</span>
<span id="cb154-10"><a href="machine-learning-with-scikit-learn.html#cb154-10"></a></span>
<span id="cb154-11"><a href="machine-learning-with-scikit-learn.html#cb154-11"></a><span class="kw">def</span> display_scores(scores):</span>
<span id="cb154-12"><a href="machine-learning-with-scikit-learn.html#cb154-12"></a>    <span class="bu">print</span>(<span class="st">&quot;Scores:&quot;</span>, scores)</span>
<span id="cb154-13"><a href="machine-learning-with-scikit-learn.html#cb154-13"></a>    <span class="bu">print</span>(<span class="st">&quot;Mean:&quot;</span>, scores.mean())</span>
<span id="cb154-14"><a href="machine-learning-with-scikit-learn.html#cb154-14"></a>    <span class="bu">print</span>(<span class="st">&quot;Standard deviation:&quot;</span>, scores.std())</span>
<span id="cb154-15"><a href="machine-learning-with-scikit-learn.html#cb154-15"></a></span>
<span id="cb154-16"><a href="machine-learning-with-scikit-learn.html#cb154-16"></a>display_scores(tree_rmse_scores)</span></code></pre></div>
<p>The <strong>Decision Tree</strong> has a score of approximately 71,407 with +/- 2,439.</p>
<p>Let’s compute the same scores for <strong>Linear Regression</strong>.</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb155-1"><a href="machine-learning-with-scikit-learn.html#cb155-1"></a>lin_scores <span class="op">=</span> cross_val_score(lin_reg, df_prepared, df_labels,scoring<span class="op">=</span><span class="st">&quot;neg_mean_squared_error&quot;</span>, cv<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb155-2"><a href="machine-learning-with-scikit-learn.html#cb155-2"></a></span>
<span id="cb155-3"><a href="machine-learning-with-scikit-learn.html#cb155-3"></a>lin_rmse_scores <span class="op">=</span> np.sqrt(<span class="op">-</span>lin_scores)</span>
<span id="cb155-4"><a href="machine-learning-with-scikit-learn.html#cb155-4"></a>display_scores(lin_rmse_scores)</span></code></pre></div>
<p>The <code>Decision Tree</code> model seems to perform <strong>worse</strong> than the <code>Linear Regression</code> model due to the ovefitting.</p>
<p>Let’s try another model: <code>RandomForestRegressor</code>. Random Forest works by training many Decision Trees on random subsets of the features, than averaging out their predictions. It is an <code>Ensemble Learning</code> model, because it builds a model on top of many other models.</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb156-1"><a href="machine-learning-with-scikit-learn.html#cb156-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb156-2"><a href="machine-learning-with-scikit-learn.html#cb156-2"></a></span>
<span id="cb156-3"><a href="machine-learning-with-scikit-learn.html#cb156-3"></a>forest_reg <span class="op">=</span> RandomForestRegressor(n_estimators<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb156-4"><a href="machine-learning-with-scikit-learn.html#cb156-4"></a>forest_reg.fit(df_prepared, df_labels)</span>
<span id="cb156-5"><a href="machine-learning-with-scikit-learn.html#cb156-5"></a></span>
<span id="cb156-6"><a href="machine-learning-with-scikit-learn.html#cb156-6"></a>df_predictions <span class="op">=</span> forest_reg.predict(df_prepared)</span>
<span id="cb156-7"><a href="machine-learning-with-scikit-learn.html#cb156-7"></a>forest_mse <span class="op">=</span> mean_squared_error(df_labels, df_predictions)</span>
<span id="cb156-8"><a href="machine-learning-with-scikit-learn.html#cb156-8"></a></span>
<span id="cb156-9"><a href="machine-learning-with-scikit-learn.html#cb156-9"></a>forest_rmse <span class="op">=</span> np.sqrt(forest_mse)</span>
<span id="cb156-10"><a href="machine-learning-with-scikit-learn.html#cb156-10"></a>forest_rmse</span></code></pre></div>
<div class="sourceCode" id="cb157"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb157-1"><a href="machine-learning-with-scikit-learn.html#cb157-1"></a>forest_scores <span class="op">=</span> cross_val_score(forest_reg, df_prepared, df_labels,scoring<span class="op">=</span><span class="st">&quot;neg_mean_squared_error&quot;</span>, cv<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb157-2"><a href="machine-learning-with-scikit-learn.html#cb157-2"></a>forest_rmse_scores <span class="op">=</span> np.sqrt(<span class="op">-</span>forest_scores)</span>
<span id="cb157-3"><a href="machine-learning-with-scikit-learn.html#cb157-3"></a></span>
<span id="cb157-4"><a href="machine-learning-with-scikit-learn.html#cb157-4"></a>display_scores(forest_rmse_scores)</span></code></pre></div>
<div class="sourceCode" id="cb158"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb158-1"><a href="machine-learning-with-scikit-learn.html#cb158-1"></a>scores <span class="op">=</span> cross_val_score(lin_reg, df_prepared, df_labels, scoring<span class="op">=</span><span class="st">&quot;neg_mean_squared_error&quot;</span>, cv<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb158-2"><a href="machine-learning-with-scikit-learn.html#cb158-2"></a></span>
<span id="cb158-3"><a href="machine-learning-with-scikit-learn.html#cb158-3"></a>pd.Series(np.sqrt(<span class="op">-</span>scores)).describe()</span></code></pre></div>
<p>Let’s try another model: <code>Support Vector Machine</code>.</p>
<p>Below the SVR (Epsilon-Support Vector Regression) model is used with the <code>kernel=linear</code> parameter.</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb159-1"><a href="machine-learning-with-scikit-learn.html#cb159-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVR</span>
<span id="cb159-2"><a href="machine-learning-with-scikit-learn.html#cb159-2"></a></span>
<span id="cb159-3"><a href="machine-learning-with-scikit-learn.html#cb159-3"></a>svm_reg <span class="op">=</span> SVR()</span>
<span id="cb159-4"><a href="machine-learning-with-scikit-learn.html#cb159-4"></a>svm_reg.fit(df_prepared, df_labels)</span>
<span id="cb159-5"><a href="machine-learning-with-scikit-learn.html#cb159-5"></a>df_predictions <span class="op">=</span> svm_reg.predict(df_prepared)</span>
<span id="cb159-6"><a href="machine-learning-with-scikit-learn.html#cb159-6"></a>svm_mse <span class="op">=</span> mean_squared_error(df_labels, df_predictions)</span>
<span id="cb159-7"><a href="machine-learning-with-scikit-learn.html#cb159-7"></a></span>
<span id="cb159-8"><a href="machine-learning-with-scikit-learn.html#cb159-8"></a>svm_rmse <span class="op">=</span> np.sqrt(svm_mse)</span>
<span id="cb159-9"><a href="machine-learning-with-scikit-learn.html#cb159-9"></a>svm_rmse</span></code></pre></div>
<div class="sourceCode" id="cb160"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb160-1"><a href="machine-learning-with-scikit-learn.html#cb160-1"></a>svm_scores <span class="op">=</span> cross_val_score(svm_reg, df_prepared, df_labels,scoring<span class="op">=</span><span class="st">&quot;neg_mean_squared_error&quot;</span>, cv<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb160-2"><a href="machine-learning-with-scikit-learn.html#cb160-2"></a>svm_rmse_scores <span class="op">=</span> np.sqrt(<span class="op">-</span>svm_scores)</span>
<span id="cb160-3"><a href="machine-learning-with-scikit-learn.html#cb160-3"></a></span>
<span id="cb160-4"><a href="machine-learning-with-scikit-learn.html#cb160-4"></a>display_scores(svm_rmse_scores)</span></code></pre></div>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data-visualization.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="clustering.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": false,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "times",
"size": 10
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Py-Book.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true,
"toolbar": {
"position": "fixed"
}
},
"css": "chapter/html/style.css",
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
